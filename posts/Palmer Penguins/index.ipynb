{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Palmer Penguins\n",
    "author: Julia Joy\n",
    "date: '2024-02-18'\n",
    "description: \"Blog Post 1: Palmer Penguins\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract \n",
    "The aim of this exploration of the Palmer Penguins dataset is to gain insight into the dataset through predictive classification. This exploration involves constructing visualizations of the data using seaborn and creating informative summary tables using pandas. By testing various combinations of models through a reproducible process, three key features are able to be identified. Out of the three features, I found that the combination of 1 qualitative feature: Clutch Completion, and 2 quantitative features: Culmen Length and Culmen Depth, were the most effective to reach 100% testing classification accuracy using a Decision Tree Model. The evaluation phase provides a visual representation of the models behavior by showing the decision regions of the trained decision tree model. Through this process, we gain a comprehensive understanding of the Palmer Penguins dataset and demonstrate the effectiveness of a trained decision tree model in accurately classifying Palmer Penguin data. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\n",
    "train = pd.read_csv(train_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a peek at the data to get a better idea of where to start, by using .head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "In order for smooth application of our data into the models we will later use to predict penguin species classification, we must prepare the data. In this case, categorical data must be one-hot-encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train[\"Species\"])\n",
    "\n",
    "def prepare_data(df):\n",
    "  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n",
    "  df = df[df[\"Sex\"] != \".\"]\n",
    "  df = df.dropna()\n",
    "  y = le.transform(df[\"Species\"])\n",
    "  df = df.drop([\"Species\"], axis = 1)\n",
    "  df = pd.get_dummies(df)\n",
    "  return df, y\n",
    "\n",
    "X_train, y_train = prepare_data(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore \n",
    "To gain a better understanding of our data, we can explore it via various methods, by constructing summary tables and visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing different summary tables \n",
    "AvgBodyMass = train.groupby([\"Species\", \"Island\", \"Sex\"]).aggregate({'Body Mass (g)' : 'mean'})\n",
    "AvgCulmen = train.groupby([\"Species\", \"Island\", \"Sex\"]).aggregate({'Culmen Length (mm)' : 'mean', 'Culmen Depth (mm)' : 'mean'})\n",
    "AvgFlipper = train.groupby([\"Species\", \"Island\", \"Sex\"]).aggregate({'Flipper Length (mm)' : 'mean'})\n",
    "#AvgCulmen2 is a table of the features I eventually found to be the best \n",
    "AvgCulmen2 = train.groupby([\"Species\", \"Clutch Completion\"]).aggregate({'Culmen Length (mm)' : 'mean', 'Culmen Depth (mm)' : 'mean'})\n",
    "\n",
    "\n",
    "print(\"Average Measurements by Species, Island, and Sex\")\n",
    "print(AvgCulmen2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n",
    "\n",
    "\n",
    "p1 = sns.scatterplot(train, x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", hue = \"Clutch Completion\", ax = ax[0])\n",
    "p2 = sns.scatterplot(train, x = \"Culmen Depth (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", ax = ax[1])\n",
    "\n",
    "ax[0].set_title(\"Culmen Depth & Length (mm) by Clutch Completion\")\n",
    "ax[1].set_title(\"Culmen Depth & Length (mm) by Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure Findings \n",
    "\n",
    "Various summary tables and visualizations were tested, aiming to determine the most effective presentation of the Palmer Penguins dataset. Initially different grouping criteria and variables were considered, such as species, island, sex, clutch completion, culmen depth and length, body mass, and flipper length. Ultimately I decided to come back and choose the summary table and visualizations that reflected the variables I later found to have the highest predictive accuracy: Culmen Depth, Culmen Length, and Clutch Completion. Summary tables were created using pandas.groupby().aggregate. From the summary tables we observe that among different species, Culmen Length varies to a larger degree than Culmen Depth does. Upon a glance of the summary table, it is hard to see if clutch completion and culmen characteristics are correlated. In the first visualization on the right, we can see the penguins depicted by virtue of the respective culmen length and depth, colored by the Clutch Completion. We observe a relatively uniform distribution of clutch completion- there does not seem to be an immediate observable relationship between the three. My next visualization shows the same axes of Culmen Depth and Length, this time however colored by species. In this figure, we notice clear species groupings, corresponding to their Culmen Depth-Length relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Best Features for Prediction\n",
    "\n",
    "In order to ensure our model has the highest classification accuracy possible, we need to select features that are more indicative of the species group a penguin belongs to. By testing every combination of features, we are able to find the three that result in the highest training set accuracy. We find that there are multiple 3-feature sets with 100% training set classification accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# these are not actually all the columns: you'll \n",
    "# need to add any of the other ones you want to search for\n",
    "all_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\n",
    "all_quant_cols = ['Flipper Length (mm)','Culmen Length (mm)', 'Culmen Depth (mm)']\n",
    "\n",
    "for qual in all_qual_cols: \n",
    "  qual_cols = [col for col in X_train.columns if qual in col ]\n",
    "  for pair in combinations(all_quant_cols, 2):\n",
    "    cols = qual_cols + list(pair) \n",
    "    print(cols)\n",
    "    # you could train models and score them here, keeping the list of \n",
    "    # columns for the model that has the best score. \n",
    "    #DT = DecisionTreeClassifier()\n",
    "    #DT.fit(X_train[cols], y_train)\n",
    "    #print(DT.score(X_train[cols], y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to narrow these feature-sets down to the single set that will be the most effective and give us not only 100% training set classificaiton accuracy but also 100% testing set classification accuracy, we repeat the process from earlier. Now operating on the testing set, we narrow down the 3-feature sets into one final best feature set: Clutch Completion, Culmen Length (mm), and Culmen Depth (mm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretend we're numbering these from 1-9, look at testing accuracy for each one that has 1.0. \n",
    "\n",
    "tempCols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train[tempCols], y_train)\n",
    "print(DT.score(X_train[tempCols], y_train))\n",
    "\n",
    "test_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\n",
    "test = pd.read_csv(test_url)\n",
    "\n",
    "X_test, y_test = prepare_data(test)\n",
    "DT.score(X_test[tempCols], y_test)\n",
    "\n",
    "\n",
    "#1 0.97\n",
    "#2 nope\n",
    "#3 1.0 !!!  ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n",
    "#4 0.97\n",
    "#5 nope\n",
    "#6 0.98\n",
    "#7 0.98\n",
    "#nope\n",
    "#9 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check our cross-validation score, noting that a cross validation score of 94.5% is indicitive of future model generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores_DT = cross_val_score(DT, X_train, y_train, cv=5)\n",
    "cv_scores_DT.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Decision Regions\n",
    "\n",
    "Finally, we can visualize these Decision Regions using matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def plot_regions(model, X, y):\n",
    "    \n",
    "    x0 = X[X.columns[0]]\n",
    "    x1 = X[X.columns[1]]\n",
    "    qual_features = X.columns[2:]\n",
    "    \n",
    "    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n",
    "\n",
    "    # create a grid\n",
    "    grid_x = np.linspace(x0.min(),x0.max(),501)\n",
    "    grid_y = np.linspace(x1.min(),x1.max(),501)\n",
    "    xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "    XX = xx.ravel()\n",
    "    YY = yy.ravel()\n",
    "\n",
    "    for i in range(len(qual_features)):\n",
    "      XY = pd.DataFrame({\n",
    "          X.columns[0] : XX,\n",
    "          X.columns[1] : YY\n",
    "      })\n",
    "\n",
    "      for j in qual_features:\n",
    "        XY[j] = 0\n",
    "\n",
    "      XY[qual_features[i]] = 1\n",
    "\n",
    "      p = model.predict(XY)\n",
    "      p = p.reshape(xx.shape)\n",
    "      \n",
    "      \n",
    "      # use contour plot to visualize the predictions\n",
    "      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n",
    "      \n",
    "      ix = X[qual_features[i]] == 1\n",
    "      # plot the data\n",
    "      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n",
    "      \n",
    "      axarr[i].set(xlabel = X.columns[0], \n",
    "            ylabel  = X.columns[1], \n",
    "            title = qual_features[i])\n",
    "      \n",
    "      patches = []\n",
    "      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n",
    "        patches.append(Patch(color = color, label = spec))\n",
    "\n",
    "      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n",
    "      \n",
    "      plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing decision regions on train set \n",
    "plot_regions(DT, X_train[keepCols], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing decision regions on test set\n",
    "plot_regions(DT, X_test[keepCols], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to truncate X_test to have only the columns we're using to predict before doing confusion matrix\n",
    "X_test = X_test[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test_pred = DT.predict(X_test)\n",
    "y_test_pred\n",
    "\n",
    "C = confusion_matrix(y_test, y_test_pred)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the trained model has 100% training and testing classifcation accuracy, the confusion matrix reflects the lack of classification errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Discussion \n",
    "\n",
    "In conclusion, this exploration of the Palmer Penguins dataset has provided insights into the dataset through predictive classification. Through the construction of informative summary tables and visually appealing figures, we were able to uncover various patterns, trends, and relationships within the data. By systematically testing different combinations of both qualitative and quantitative features of the Palmer Penguins, I found the best combination of three features that yielded high classification accuracy. One qualitative feature, clutch completion, and two quantitative features, culmen length and culmen depth (mm). By employing scikit-learn's DecisionTreeClassifier, I trained a model on these features and achieved a classification accuracy of 100% on both the training and testing sets. This shows the importance of feature selection, model selection and evaluation in building accurate predictive models. Through this whole process, I gained a comprehensive understanding of the Palmer Penguins dataset and demonstrate the effectiveness of a trained model in accurately classifying Palmer Penguin data. From this process, I learned how important both the selection of the right features as well as the right model are. I learned that 100% training set classification accuracy does not guarantee 100% testing set classification accuracy, and how it is important to tweak the model to ensure that future uses beyond a training set maintain a high level of accuracy. I gained an appreciation of visualizations as a tool for greater understanding of initial data as well as the effect of models. I feel confident in future endeavors to explore, visualize, and predict data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "debe06cc0f9553f110b64dc3926c05df82dae2145b852c8422b9c04315589dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
