[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Julia Joy."
  },
  {
    "objectID": "posts/GoalSetting/index.html",
    "href": "posts/GoalSetting/index.html",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Julia Joy\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI think that the areas that i would most like to focus on are primarily the Implementation. I always feel like more practice with effective coding and learning how to use new tools is so invaluable, I am excited to continue that in this course, especially with all the different kinds of Machine Learning algorithms and models. I am also interested in the Theory aspect as a mathematics minor, I love learning about the numbers and formulas that go behind the operations we can take for granted when we code. I never took multi-calculus (I know, odd for a math minor), so while I think it will be a bit challenging to dredge up my calc and linear algebra since I took those courses so long ago, I think it will be rewarding and fun to expand my knowledge.\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI think blog posts will be pretty crucial to my learning in this course. Already with the first blog post and working on the second, I enjoy taking the content we discuss in class and applying it on my own. I have always felt like a hands-on learning style fits me the best. Ideally I would like to have a “Satisfactory/No Revision Required” rating on 5-6 of my of my blog posts, and perhaps completing more (I want to complete at least around 60-80% of blog posts) but leaving some space for some blog posts to remain in the “needs revision category”. I have realized that I am someone who is very driven by interest when it comes to coursework. Assignments I enjoy are fun little challenges for me, and I will spend hours working and tweaking them to my satisfiability even when they’re already probably pass the threshold of “good enough” grade wise. But assignments with content that I am not as passionate or interested about can be really hard for me to motivate to spend extra time on, so I would enjoy leaving some space for myself to say “no I think that is good enough for me. I tried it out and am satisfied with my work.”. I aim to give myself about 1-1.5 weeks for a fresh first-draft submission of each blog post I do, just to keep myself on a semi-timeline to ensure nothing gets crammed in at the end. With revision, I think that will be more dependent on extra time I have each week, spending more time on it in the weeks I have a bit more extra time and vice versa, I will allow myself less of a set timeline with those.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI strive to attend every class, with the exception of one to two when it is truly necessary to miss class, such as if I’m sick, have to be out of town for some reason, or if my mental health gets in the way of my academics. In our two weeks of class, I have already gained an appreciation for the daily warmups. I find myself actually spending a good chunk of time on them before class, but I feel like it really does benefit me. The warmups prep me and lay a groundwork for either just basic understanding for the subject we tackle in that class, or in the application of what we’re about to talk about. I aim to complete every warmup prior to class this semester, with the leeway of one or two because life does happen. Outside of completing each warmup, whenever we are in our small groups, I aim to be very involved and ask questions and offer suggestions/solutions to whoever is the warmup presenter. I hope to complete most of the course readings, using it as more of a supplement to my hands on learning. Additionally, I notice that I frequently find it challenging to speak up in class, especially in a large class full of many people who are not only my peers but also my friends. The warmups in small groups help ease the transition before I potentially have to speak in front of the whole class. While I do still find it a bit intimidating, I really appreciate and value being able to talk the problems out in a smaller group setting before a larger one. Peer help/TA/office hours have long time been my favorite way to address questions or concepts I have a hard time grasping, so I expect to attend those when the need strikes. I have realized I am much more of a ask-questions-in-person kind of person than a slack questions user, so while I will check the slack and at some point probably use it to send a chat myself, I do not expect frequent use of it for myself. I think study groups will be big for me this semester, as I am familiar with many people in this course, so I think that will also be beneficial in my learning if I get stuck anywhere and help hours are immediately around.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI am excited about this project as I feel like it will allow me to explore what machine learning can be in an area that I’m a bit more interested in, whatever that may be. Without knowing exactly what my project will look like or be about I cannot detail my goals in that way, however I know some general ideas of what I would like to accomplish. I would like to complete all project milestones on time to keep myself and the group accountable in our semester time frame, contribute to the project in an equal manner including in the algorithm implementation and project presentation, and do what I can to be a great group project partner. I think that everyone has strengths and weaknesses, and for example while I don’t think that I’m the best at writing automated checks for algorithms written by others, I have other skills to offer. I think that is the most important thing about working in a team, to acknowledge everyone’s weaknesses and capitalize on peoples strengths."
  },
  {
    "objectID": "posts/GoalSetting/index.html#what-youll-learn",
    "href": "posts/GoalSetting/index.html#what-youll-learn",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nI think that the areas that i would most like to focus on are primarily the Implementation. I always feel like more practice with effective coding and learning how to use new tools is so invaluable, I am excited to continue that in this course, especially with all the different kinds of Machine Learning algorithms and models. I am also interested in the Theory aspect as a mathematics minor, I love learning about the numbers and formulas that go behind the operations we can take for granted when we code. I never took multi-calculus (I know, odd for a math minor), so while I think it will be a bit challenging to dredge up my calc and linear algebra since I took those courses so long ago, I think it will be rewarding and fun to expand my knowledge."
  },
  {
    "objectID": "posts/GoalSetting/index.html#what-youll-achieve",
    "href": "posts/GoalSetting/index.html#what-youll-achieve",
    "title": "CSCI 0451: Reflective Goal-Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI think blog posts will be pretty crucial to my learning in this course. Already with the first blog post and working on the second, I enjoy taking the content we discuss in class and applying it on my own. I have always felt like a hands-on learning style fits me the best. Ideally I would like to have a “Satisfactory/No Revision Required” rating on 5-6 of my of my blog posts, and perhaps completing more (I want to complete at least around 60-80% of blog posts) but leaving some space for some blog posts to remain in the “needs revision category”. I have realized that I am someone who is very driven by interest when it comes to coursework. Assignments I enjoy are fun little challenges for me, and I will spend hours working and tweaking them to my satisfiability even when they’re already probably pass the threshold of “good enough” grade wise. But assignments with content that I am not as passionate or interested about can be really hard for me to motivate to spend extra time on, so I would enjoy leaving some space for myself to say “no I think that is good enough for me. I tried it out and am satisfied with my work.”. I aim to give myself about 1-1.5 weeks for a fresh first-draft submission of each blog post I do, just to keep myself on a semi-timeline to ensure nothing gets crammed in at the end. With revision, I think that will be more dependent on extra time I have each week, spending more time on it in the weeks I have a bit more extra time and vice versa, I will allow myself less of a set timeline with those.\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI strive to attend every class, with the exception of one to two when it is truly necessary to miss class, such as if I’m sick, have to be out of town for some reason, or if my mental health gets in the way of my academics. In our two weeks of class, I have already gained an appreciation for the daily warmups. I find myself actually spending a good chunk of time on them before class, but I feel like it really does benefit me. The warmups prep me and lay a groundwork for either just basic understanding for the subject we tackle in that class, or in the application of what we’re about to talk about. I aim to complete every warmup prior to class this semester, with the leeway of one or two because life does happen. Outside of completing each warmup, whenever we are in our small groups, I aim to be very involved and ask questions and offer suggestions/solutions to whoever is the warmup presenter. I hope to complete most of the course readings, using it as more of a supplement to my hands on learning. Additionally, I notice that I frequently find it challenging to speak up in class, especially in a large class full of many people who are not only my peers but also my friends. The warmups in small groups help ease the transition before I potentially have to speak in front of the whole class. While I do still find it a bit intimidating, I really appreciate and value being able to talk the problems out in a smaller group setting before a larger one. Peer help/TA/office hours have long time been my favorite way to address questions or concepts I have a hard time grasping, so I expect to attend those when the need strikes. I have realized I am much more of a ask-questions-in-person kind of person than a slack questions user, so while I will check the slack and at some point probably use it to send a chat myself, I do not expect frequent use of it for myself. I think study groups will be big for me this semester, as I am familiar with many people in this course, so I think that will also be beneficial in my learning if I get stuck anywhere and help hours are immediately around.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI am excited about this project as I feel like it will allow me to explore what machine learning can be in an area that I’m a bit more interested in, whatever that may be. Without knowing exactly what my project will look like or be about I cannot detail my goals in that way, however I know some general ideas of what I would like to accomplish. I would like to complete all project milestones on time to keep myself and the group accountable in our semester time frame, contribute to the project in an equal manner including in the algorithm implementation and project presentation, and do what I can to be a great group project partner. I think that everyone has strengths and weaknesses, and for example while I don’t think that I’m the best at writing automated checks for algorithms written by others, I have other skills to offer. I think that is the most important thing about working in a team, to acknowledge everyone’s weaknesses and capitalize on peoples strengths."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html",
    "href": "posts/MedicalBiasJLS/PostHomePage.html",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "",
    "text": "In this project we explore the implementation of a machine learning model designed to make predictions concerning health outcomes. We aim to train and analyze the results of these models. We ultimately hope to identify bias from the risk scores, while acknowledging that differences in illness rates can naturally vary and are not necessarily indicative of bias. We use two model types (decision trees and random forests), found to be the optimal performers for our data, to generate risk scores for every patient in a synthetic medical data set created by Synthea (Hoyt and Muenchen 2019). We chose six condition “groups” to study, combining data from multiple related conditions into one has/does not have target feature. These groups included pulmonary diseases, diabetes and comorbidities, cardiovascular diseases, pregnancy and pregnancy complications, and cancer. We then interpreted risk factors for these categories across race, ethnicity, birthplace, and current town of residence. We then compared and analyzed risk scores across identities and illness categories. A significant portion of our results showed that patients of colors, and patients born in or residing in less wealthy towns have higher risk factors for some conditions, pointing to the influence of environmental and social inequity factors. Some results, however, were more evenly spaced and harder to interpret. Our findings reveal disparities in risk factors among different demographics, emphasizing the impact of environmental and social inequities on health outcomes and the need for further investigation and analysis."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html#abstract",
    "href": "posts/MedicalBiasJLS/PostHomePage.html#abstract",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "",
    "text": "In this project we explore the implementation of a machine learning model designed to make predictions concerning health outcomes. We aim to train and analyze the results of these models. We ultimately hope to identify bias from the risk scores, while acknowledging that differences in illness rates can naturally vary and are not necessarily indicative of bias. We use two model types (decision trees and random forests), found to be the optimal performers for our data, to generate risk scores for every patient in a synthetic medical data set created by Synthea (Hoyt and Muenchen 2019). We chose six condition “groups” to study, combining data from multiple related conditions into one has/does not have target feature. These groups included pulmonary diseases, diabetes and comorbidities, cardiovascular diseases, pregnancy and pregnancy complications, and cancer. We then interpreted risk factors for these categories across race, ethnicity, birthplace, and current town of residence. We then compared and analyzed risk scores across identities and illness categories. A significant portion of our results showed that patients of colors, and patients born in or residing in less wealthy towns have higher risk factors for some conditions, pointing to the influence of environmental and social inequity factors. Some results, however, were more evenly spaced and harder to interpret. Our findings reveal disparities in risk factors among different demographics, emphasizing the impact of environmental and social inequities on health outcomes and the need for further investigation and analysis."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html#introduction",
    "href": "posts/MedicalBiasJLS/PostHomePage.html#introduction",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Introduction",
    "text": "Introduction\nFor our project, we aim to explore the relationship between diseases and social factors such as sex, race, and town, and how these may reflect societal and environmental inequities. We understand from academic literature like “Diabetes Complications in Racial and Ethnic Minority Populations in the USA” (Haw et al. 2021) and “The Black-White Disparity in Pregnancy-Related Mortality from 5 Conditions: Differences in Prevalence and Case-Fatality Rates” (Tucker et al. 2007) that there exist severe inequities in risk rates across different identity groups. Building on this literature, we are interested in building a machine learning model that reveal these patterns in the US healthcare system.\nOur approach is to identify the most accurate predictive model for our dataset, then use this model to generate risk likelihood scores and evaluate the relationship between different diseases and characteristics indicative of societal inequalities. We will then analyze the implications of these risk factors for inequitable, identity-based risk factors in health outcomes and complications. Our project consists of three documents, one in which we clean our original data, one in which we explore this data visually, one in which we build and explore our models, and the final one, this one, in which we describe our motivations, background, results, and analysis. Taking the general trends we witness in our data visualization document, we carried out the second half of our project; building a model that predicts risk scores.\nWe were inspired by Obermeyer and colleagues work (Obermeyer et al. 2019) in analyzing the bias present in machine learning models used to guide healthcare decisions by using health costs as a proxy measure for health. This model used pre-existing bias in our healthcare system to make decisions that further marginalized oppressed identity groups. Our ultimate goal is that our models will bring to light existing inequities. Comparing the risk scores, we analyze whether trends emerged in terms of socioeconomic status (which we measure by the proxy of town of birth and residence), race, gender, and ethnicity. We then used Barocas et al.’s paper (Barocas, Hardt, and Narayanan 2023) to analyze the implications of our findings in terms of the three definitions of fairness. Finally, we referenced articles like “Explanatory learner models: Why machine learning (alone) is not the answer” (Rosé et al., n.d.) to analyze the potential negative impact of relying on machine learning models in important decision contexts and proposed solutions to this dilemma."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html#values-statement",
    "href": "posts/MedicalBiasJLS/PostHomePage.html#values-statement",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Values Statement",
    "text": "Values Statement\nThe motivation behind our project was to uncover potential inequities in the manifestations of certain conditions, for example does a persons race or socioeconomic status predispose them to certain conditions more than others. Our goal was to identify potential societal and environmental factors that unjustly, or disproportionately contribute to disparities in health outcomes. Our focus on this project stems from a desire to understand and address societal and evironmental inequities that contribute to disparities in health outcomes, and our personal commitments to promoting equity and social justice in healthcare.\nThe primary potential users of our project would include researchers, policymakers, and public health organizations interested in understanding and addressing health inequities. However, the project’s findings and potential implications could also affect the communities we study, especially those that we find experience disparities in health outcomes due to social determinants.\nIf our research were to be taken out of context by researchers and health professionals, and taken to be a study of biological predisposition, and not of the manifestation of social factors, our results may reinforce assumptions about health outcomes by race and ethnicity in the medical field, enforcing harmful stereotypes or leading to further marginalization of certain groups. Additionally, if the data or models have inherent biases, they could perpetuate or amplify existing disparities.\nWith proper usage and implementation though, we hope our results would positively impact public health programs and initiatives that work in preventative measures in the most at-risk communities. With our data, we hope that these measures would more easily identify communities in which to center efforts and awareness campaigns, by shedding light on health inequities and informing efforts to address them."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html#material-methods",
    "href": "posts/MedicalBiasJLS/PostHomePage.html#material-methods",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Material & Methods",
    "text": "Material & Methods\n\nOur Data\nOur project utilizes a synthetic data set created for an Introduction to Biomedical Data Science Textbook, (Hoyt and Muenchen 2019). The data was created using Synthea, a synthetic patient generator that models the medical history of synthetic patients. Synthea’s mission is “to output high-quality synthetic, realistic but not real, patient data and associated health records covering every aspect of healthcare.” This allowed for much easier access than real patient data, as well as alleviating any privacy concerns that would arise from using real patient data. The link to the data can be found here: https://data.world/siyeh/synthetic-medical-data.\nOur dataset was originally quite large, with over 200 million entries. After thorough data cleaning and preprocessing, the data was then transformed to multiple CSV documents, generally with the format of each row representing a different patient with one-hot-encoded values for multiple disease conditions.\nWhile using synthetic data has its benefits, it is essential to acknowledge certain inherent limitations. Firstly, despite efforts to create diverse and representative synthetic patients, there may still be discrepancies in representing certain demographic groups or medical conditions accurately. Certain rare or uncommon medical conditions may be underrepresented in the dataset due to the limitations of the modeling and analyses processes. This data is generated to represent patients from Massachusetts, so contains a population representative of this state which is primarily White, wealthy, and educated and does not accurately represent the diversity of the rest of the Unites States. Therefore, any generalization of results must proceed with caution. Thus while this synthetic dataset serves as a valuable resource for educational purposes, researchers and practitioners should approach its use with an understanding of its limitations.\nAfter cleaning our data, we performed exploratory data analysis in order to visualize out dataset, the results of which are found in this extension of our materials and methods section, our exploratory data analysis section.\n\n\nOur Approach\nSince our original dataset was quite large, a thorough procedure of data cleaning and preprocessing was needed, as well as an evaluation of which parts and features of our data should be actively used as predictors for our models. We subset our data into different CSV files, each entry to a given CSV corresponding to the different category of condition. This allowed for our models to be trained more concisely and efficiently, as well as increased the interpretability of results. This extension of methods and materials shows our data cleaning process in more depth.\nMultiple models were trained using cross-validation for each analysis of a condition group, including a logistic regression model, a decision tree classifier, a random forest classifier, and a support vector machine. These models were then evaluated for best score for a specific condition group. The best model, i.e. the one returning the highest cross-validated accuracy, was chosen as the predictive model for our general risk scores. We then trained this optimal model on our training dataset, and created predictions for our testing data that represented the probability of each entry being 1 (having a certain condition) or 0 (not having a certain condition). A risk score could then be anything between 0.00 and 1.00, where 0.50 would represent a 50% probability that the given patient has a condition. The models ran on our own personal devices, on the ML-0451 class kernel."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html#results",
    "href": "posts/MedicalBiasJLS/PostHomePage.html#results",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Results",
    "text": "Results\nThe first extension of our results section is this document, in which we run our model and generate the risk scores and comparisons we discuss in further depth here.\nThe results of our risk score predictions varied widely. It is important to remember that the foundation of our project is based on interpreting our model’s perceptions of different group’s likelihoods of having a certain condition, and so they might reasonably disagree with actual trends in condition prevalence. With this in mind, in this section we will delve more deeply into the findings of our model.\nFirst, we wanted to inspect prevalence by race, to understand whether one racial group was more often assigned higher risk scores than others across conditions.\n\nResults by Race:\n\n# importing results for visualization\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nrace_risk = pd.DataFrame()\n\nrace_risk['Race'] = ['White', 'Black', 'Hispanic', 'Asian']\nrace_risk['Diabetes'] = [0.312536, 0.256158, 0.340659, 0.479592]\nrace_risk['Pregnancy'] = [0.034260, 0.051395, 0.038217, 0.037262]\nrace_risk['Cancer'] = [0.051942, 0.046859, 0.051650, 0.034009]\nrace_risk['Heart'] = [0.502101, 0.491353, 0.491280, 0.510746]\nrace_risk['Lung'] = [0.507817, 0.509092, 0.492106, 0.514338]\n\nprint(race_risk)\nprint('Fig. 1. Table of predicted risk score values by race for each condition subset.')\n\n       Race  Diabetes  Pregnancy    Cancer     Heart      Lung\n0     White  0.312536   0.034260  0.051942  0.502101  0.507817\n1     Black  0.256158   0.051395  0.046859  0.491353  0.509092\n2  Hispanic  0.340659   0.038217  0.051650  0.491280  0.492106\n3     Asian  0.479592   0.037262  0.034009  0.510746  0.514338\nFig. 1. Table of predicted risk score values by race for each condition subset.\n\n\n\n# plotting\ntransposed_race_risk = race_risk.set_index('Race').T\ntransposed_race_risk.plot(kind=\"bar\")\n\nplt.title('Bar Chart of Health Risks by Race')\nplt.xlabel('Race')\nplt.ylabel('Risk Score %')\n# scale y axis by 100 to show percentage\nplt.yscale('linear')  \nplt.ylim(0, 1) \n\nplt.xticks(rotation=45)\nplt.show()\n\nprint('Fig. 2. Bar chart of predicted risk score values by race for each condition subset. Risk scores are given in percentage.')\n\n\n\n\n\n\n\n\nFig. 2. Bar chart of predicted risk score values by race for each condition subset. Risk scores are given in percentage.\n\n\nFrom this table and plot we can see that risk scores are generally evenly-distributed across racial categories, meaning our model does not take one racial identity as a strong risk factor for all diseases indiscriminately.\nWhat was interesting as we generated our predictions was that both heart and lung disease both predicted near equal risk scores, all around 50%, for all racial groups. This finding disagrees with real medical literature, which shows that Black patients are statistically more likely to experience any pulmonary defects (A. T. Moffett 2021). Part of what could explain our high risk for lung disease across the board, and the lack of differentiation by race we would expect to see, could be due to the fact that we included a large variety of pulmonary diseases, including seasonal allergies and acute bacterial sinusitis. The latter of these, a bacterial sinus infection, is an incredibly common condition; one in 100 common colds lead to sinusitis (“Sinusitis” 2023). Especially as we ignore the fact that our data is simulated to have occurred over a long period of time (years), it is entirely reasonable to expect that, over a certain period of time, anyone may have a 50% risk of experiencing symptoms of sinusitis.\nIn terms of heart conditions, once again our model does not match trends in medical findings about cardiovascular outcomes by race, which also finds Black patients at higher risk for cardiovascular conditions (Zulqarnain Javed 2022). Unlike our lung conditions subset, the cardiovascular conditions identified are indicators of larger health issues, and not common complications of everyday illnesses. The fact that our model assigns equal scores regardless of race has large implications for the potential applications of a model like ours, built on synthetic data as ours was. If a model systematically underestimates the risk of groups compared to each other, for example our model would indicate treating all patients similarly regardless of their race, doctors and systems implementing our system may then systemically under-diagnose and treat Black patients, as they are unaware of their true increased risk compared to other racial groups.\nIn terms of diabetes, our model predicted that Asian patients are more likely to experience diabetes, prediabetes or a diabetes-related comorbidity. This agrees with medical findings, that even at lower BMIs, Asian patients are at higher risk for type-2 diabetes (“Diabetes and Asian American People” 2022). Our model also predicted that Black patients are at the highest risk for pregnancy complications, which again agrees with prevailing medical literature (Venicia Gray 2023). This is a promising result as it shows that our model may have a positive usage if implemented in healthcare settings, as it can draw attention to differentiated risks based on risk for diabetes and pregnancy complications.\n\n\nResults by Ethnicity:\nOur results for ethnicity were harder to interpret than our results by race. This following figure shows to top two most at risk ethnic groups and bottom two least at risk ethnic groups for each condition subset. We found that those in the middle often hovered around the same risk scores, and so did not provide as much valuable information for the diagnostic process.\n\ncancer = pd.DataFrame()\npreg = pd.DataFrame()\nheart = pd.DataFrame()\ndia = pd.DataFrame()\nlungs = pd.DataFrame()\n\n\ncancer['Ethnicity'] = ['German', 'Puerto Rican', 'Russian', 'Scottish']\ncancer['Risk'] = [0.105675, 0.067085, 0.005342, 0.005342]\n\npreg['Ethnicity'] = ['Dominican', 'Scottish', 'German', \"Russian\"]\npreg['Risk'] = [0.074523, 0.074523, 0.023289, 0.014905]\n\n\nheart['Ethnicity'] = ['Asian Indian', 'Scottish', 'Dominican', 'Chinese']\nheart['Risk'] = [0.556735, 0.556144, 0.480579, 0.464757]\n\ndia['Ethnicity'] = ['Asian Indian', 'Polish', 'Russian', 'Irish']\ndia['Risk'] = [0.714286, 0.558405, 0.200000, 0.182902]\n\nlungs['Ethnicity'] = ['Polish', 'Mexican', 'West Indian', 'Chinese']\nlungs['Risk'] = [0.583031, 0.580761, 0.457941, 0.451298]\n\nfig, axs = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n\nfor i, (df, title) in enumerate(zip([cancer, preg, heart, dia, lungs], ['Cancer', 'Pregnancy', 'Heart', 'Diabetes', 'Lungs'])):\n    axs[i].bar(df['Ethnicity'], df['Risk'], color='skyblue')\n    axs[i].set_title(title)\n    axs[i].tick_params(axis='x', rotation=45)\n\n# scale y axis by 100 to show percentage\nplt.yscale('linear')  \nplt.ylim(0, 1) \n\nplt.tight_layout()\nplt.show()\n\nprint('Fig. 3. The risk score, shown in percentage on the y-axis, by the top and bottom two most at-risk ethnic groups based on condition subset.')\n\n\n\n\n\n\n\n\nFig. 3. The risk score, shown in percentage on the y-axis, by the top and bottom two most at-risk ethnic groups based on condition subset.\n\n\nThe first thing we checked with these results was whether the risks were skewed based on representation, i.e. whether some ethnicity’s appear much more frequently or much less frequently in our overall dataset. If for example Russian appeared infrequently in our dataset, and had a lower score overall for many of the conditions, we could not be sure whether this ethnic Russians truly have a lower risk, or are just underrepresented in our dataset. After comparing the distributions of ethnic groups in our dataset, we found that Russian and Scottish ethnic groups seemed to be slightly underrepresented, but that overall no ethnic group appeared drastically less or more frequently than any other, making us confident that most of the driving forced behind our results are true patterns in diagnosis in our dataset, and not issues of skew.\nIn figure three, we see that our most at-risk ethnic groups by condition align with our most at-risk racial groups by condition, which is promising. The diabetes, lung, and heart conditions also all show increased risk overall, which matches our models predictions that those groups of conditions appear to be more common than cancer and pregnancy-related conditions. We hope that the figure above could serve a starting point for further investigation into the implications of increased/decreased risk for conditions based on having these ethnic identities. Without being intimately familiar with the demographic history and trends of Massachusetts, it’s hard to say whether environmental or genetic factors are the main factors influencing these results. Further research into community trends might find it valuable to draw on these scores as quantitative background for the effects of observed environmental, genetic, and societal factors\n\n\nResults by Town of Birth/Current Residence:\nThe last category we examined our data through was the town of birth and the the town current residence. We chose to compare both, as we weren’t certain whether the place of someone’s birth, or the place they currently reside (and therefore may have been residing for much of their adult lives) effects their health outcome more.\n\nbptown = pd.DataFrame()\n\nbptown['TownCat'] = ['Wealthy', 'Non-Wealthy']\nbptown['Lung'] = [0.4872429058942045, 0.5189382015534418]\nbptown['Heart'] = [0.10892307692307693, 0.1103076923076923]\nbptown['Cancer'] = [0.06399830132085002, 0.04238804096017698]\nbptown['Pregnancy'] = [0.038981469137448335, 0.03668844154112784]\nbptown['Diabetes'] =  [0.33305156382079454, 0.31947027331642713]\n\ncrtown = pd.DataFrame()\ncrtown['TownCat'] = ['Wealthy', 'Non-Wealthy']\ncrtown['Lung'] = [0.5039833131472166, 0.5124758651408807]\ncrtown['Heart'] = [0.08661538461538462, 0.09256410256410255]\ncrtown['Cancer'] = [0.03621368085712754, 0.05164958111475114]\ncrtown['Pregnancy'] = [0.04586055192640981, 0.03630627027507444]\ncrtown['Diabetes'] = [0.25274725274725274, 0.2827087442472057]\n\nprint(bptown)\nprint('Fig. 4. Average Risk Score by Condition Group According to Wealth Designation of Birthplace.')\n\n       TownCat      Lung     Heart    Cancer  Pregnancy  Diabetes\n0      Wealthy  0.487243  0.108923  0.063998   0.038981  0.333052\n1  Non-Wealthy  0.518938  0.110308  0.042388   0.036688  0.319470\nFig. 4. Average Risk Score by Condition Group According to Wealth Designation of Birthplace.\n\n\n\nprint(crtown)\nprint('Fig. 5. Average Risk Score by Condition Group According to Wealth Designation of Current Town of Residence.')\n\n       TownCat      Lung     Heart    Cancer  Pregnancy  Diabetes\n0      Wealthy  0.503983  0.086615  0.036214   0.045861  0.252747\n1  Non-Wealthy  0.512476  0.092564  0.051650   0.036306  0.282709\nFig. 5. Average Risk Score by Condition Group According to Wealth Designation of Current Town of Residence.\n\n\nFrom the tables above, we see that for most conditions, wealthy versus non-wealthy towns have relatively similar risk scores. This could be due to the lack of great variety in our dataset in terms of environment. Of course some towns are wealthier than others, but they still all exist within Massachusetts, a smaller state (area-wise) compared to most others, and therefore not many of the towns we include truly exist in their own bubble where direct correlations between town wealth and health of its inhabitants can easily be measured.\nThat being said, there are some differences worth noting. We found the disparity in pulmonary condition risk score between wealthy and non-wealthy towns being wider by birthplace interesting, as we knew from the literature that childhood asthma is often linked to environmental factors such as air quality and poverty (Christina M. Pacheco 2014). We also see that patients with a non-wealthy town of current residence are slightly more at risk for developing diabetes. This is a rational result for two reasons. Firstly, the food someone eats as a baby has less of an effect on their present risk for developing diabetes than the foods they have currently have access to. Secondly, non-wealthy towns often lack access to fresh fruits and vegetables, and other more expensive, but less-processed, food items. Interestingly, both patients with a wealthy birthplace and those with a wealthy current town are at a greater risk of pregnancy complications according to our model. Speculatively, this could be due to differences in mothers age, as younger mothers are more likely to be lower income than older mothers, but older mother are at a much higher risk for complications (K A Moore 1993). Our cancer risk is higher for those born in wealthy towns, but lower for those currently residing in a wealthy town. This second result could be explained by increased access to early-intervention/preventative healthcare and diagnostics. However, we are unsure about the significance of birthplace disparity. Furthermore, our risk for heath diseases are almost equal across birthplaces and current towns of residence."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html#concluding-discussion",
    "href": "posts/MedicalBiasJLS/PostHomePage.html#concluding-discussion",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOur project was able to accomplish our goal of analyzing risk rates for various illnesses and conditions for different identities. Due to the large quantity of data we possessed, we were unable to analyze all of the data we had access to to make predictions. Ideally, we would have been able to predict medication use or various observations in addition to specific conditions. Also, if we had access to more data, we could have made more specific predictions- like for asthma instead of general lung ailments. If we had more time, computational resources, and data we would like to extend our study to include healthcare information for different conditions as well as different geographical regions outside of Massachusetts. By amplifying the range of data we include, we would be able to come to more concrete conclusions on different risk rates. However, we were able to complete our aspirations for this project by generating risk rates for race, gender, ethnicity, birthplace, and current address for five different ailments.\nOur results compare to the results of those who have studied similar problems. For example, there is a large quantity of scientific data that shows that people at lower socio-economic status are more likely to get diabetes (Yelena Bird 2015). Furthermore, race has been strongly connected to material mortality and health. Specifically, black and hispanic women are at much higher risk of issues with pregnancy than their white counterparts (Eran Bornstein 2020). We saw both these trends and more replicated in our model’s predictions. Therefore we can conclude that our model is creating predictions that are correlated with real life trends.\n\nCritical Discussion\nThe goal of our presentation is to analyze the bias present in our healthcare system and the risk of certain groups of different illnesses and health conditions. There are many organizations that might find this type of model useful or interesting. One interested party could be a hospital that wants to allocate resources based on the communities they serve. This could be helpful as they could adapt to real community needs. A similar use case could be if a town is building or allocating healthcare resources and wants to understand the risks of their township or locality. Hopefully, this model could help allow resources to go to the places in which there is great need. However, an important note is that this dataset measures the recorded rates of a hospital setting. This could widely vary from real illness rates, as certain communities are under-treated or under-diagnosed in the US healthcare system.\nA more harmful use case would include an insurance company incorporating this model into their decisions about providing healthcare coverage. Therefore, our model has the risk, if put into the wrong hands, to have negative impacts on already marginalized communities. As insurance companies have significant resources, it is probable they would finance a project like this. Thus the question arises of whether this model should be allowed to be employed in decision-making scenarios.\nWe completed this work out of curiosity as part of an educational pursuit. If used for knowledge or understanding of the impact of different illnesses and conditions on identity groups, it can be helpful and informative. However, there is also the risk of further harming groups that have already been historically marginalized within the US healthcare system."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html#the-3-definitions-of-fairness",
    "href": "posts/MedicalBiasJLS/PostHomePage.html#the-3-definitions-of-fairness",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "The 3 Definitions of Fairness",
    "text": "The 3 Definitions of Fairness\nIn their paper (Barocas, Hardt, and Narayanan 2023), Barocas, Hardt, and Narayanan outline three relative notions of fairness: the narrow, middle, and broad views. The narrow view of fairness suggests that we should treat similar individuals in the same manner, given how currently similar they are. The broad perspective advocates for structuring society to facilitate similar outcomes for people with comparable abilities and ambitions. The middle-ground stance proposes that we treat different people equally, under the assumption that their apparent dissimilarities stem from factors just as past injustices or misfortunes that should be disregarded.\nSince our project involves comparing people of different demographic and characteristics, evaluating risk scores and examining fairness, we must look into our project and identify what is fair, as well as what we are choosing to define as fair.\nUnder the narrow view of fairness, since the comparison is between individuals and not directly concerned with the way members of specific groups might be treated, The narrow view only commands that similar people be treated similarly. In our models, similar people (eg of the same demographic factor being studied such as race, gender, ethnicity…) are being treated similarly in our models.\nUnder the middle view of fairness, since the decision makers have an obligation to avoid perpetuating injustice, our evolution of our model and data’s biases in attempts to expose the perpetuation of injustice keeps us in alignment with the middle view of fairness. However, if this model were to be used in an alternative way, say by insurance companies in deciding coverage, it could potentially violate this definition.\nThe broad view of fairness focuses on the degree to which society overall is structured to allow people of similar ability and ambition to achieve similar success. Under this definition, outcomes are solely attributed to difference in ability and ambition. However, the clear differences in risk scores for varying groups of demographics suggests this third definition is violated. An example of this is how as Black people are more likely to have pregnancy complications and men are twice as likely to get cancer. The disproportionate effects of diseases predicted by our models suggest that environmental or systemic factors may be at play. Intervention and change are needed at the basic level of societal structure to accomplish this third definition of fairness."
  },
  {
    "objectID": "posts/MedicalBiasJLS/PostHomePage.html#group-contributions",
    "href": "posts/MedicalBiasJLS/PostHomePage.html#group-contributions",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Group Contributions",
    "text": "Group Contributions\nImportant Note: The additions/deletions on Github look skewed because of the creations/deletions of the csv files, not the actual code.\nLindsey: At the beginning of the project, I worked alongside Sophie to clean the data and figure out how to merge our information without the kernel dying. This included pivoted columns, filtering the data into various conditions, and merging the datasets. The next large role I took on was creating the code to train models on the dataframes we had created in our DataCleaning file. Together, the three of us worked to create code to evaluate the models based on race, gender, ethnicity, birthplace, and current address. I took a leading role in figuring out how to evaluate birthplace risk based on wealth of cities in Massachusetts. I also worked to re-organize this evaluation code so that we could reuse it for all of the conditions we studied. In terms of the blog post writing, I completed the two discussion sections, found sources to reference in our introduction that helped us develop our analysis throughout the Blog Post, and edited the writing throughout. Although our attention was divided among various aspects of the project, we collaborated effectively as a team, supporting one another whenever any member encountered a challenge.\nJulia: In the beginning of the project, I created data visualizations allowing us to better understand our data and project question goals. The large role I took on was to create code to evaluate the models based on race, gender, ethnicity, birthplace, and current address. I implemented the evaluation of cancer, heart diseases, and lung diseases. I worked alongside with Lindsey to find and import our Massachusetts wealth information, in order to evaluate birthplace and current town residence risk based on wealth.\nI additionally went through our model code and repaired seeding and randomness issues, to ensure our models were performing to the same caliber and ‘accuracy’ across our code. Throughout the project, I took on the role of keeping our data thoroughly cleaned and organized, as we frequently found ourselves with extraneous and additional code we did not need, as well as a need to organize our code for comprehensibility and readability as we worked both separably and together. In terms of the blog post writing, I completed our Values Statement, Material & Methods section, as well as the discussion on the three views of fairness. Our efforts were comprehensive and collaborative throughout this project. Pair-programming was utilized alongside our individual divide-and-conquer. We functioned cohesively as a group, completing our project with an happy divide of labor and effort.\nSophie: Lindsey and I started by working on cleaning and merging our data into usable (i.e. not large enough to crash our kernel every time) datasets for model training. Afterwards, I contributed to the exploratory data analysis document by creating graphs showing differences in condition prevalence by race, ethnicity, and birthplace (for which I had to find the populations of each town in our dataset to calculate prevalence). We worked together on writing the code to generate models and risk scores for our conditions. I started to address the problem of random seeding in our data, which was causing variable results each time we ran, which Julia took on later. Afterwards I focused on the organization aspect of our blog post, writing explanatory comments for all our documents/code lines, and creating the format for our post in terms of linking all of our various working documents together into this one, more streamlined document. I also wrote our introduction, abstract, and results section, and Julia and I worked together on our values statement, and I wrote parts of our approach. Overall, I think we worked very well as a group in terms of division of labor and coding together. We were all proactive in taking the lead on certain aspects of the project, and worked very collaboratively together when we were stuck on certain parts."
  },
  {
    "objectID": "posts/MedicalBiasJLS/DataCleaning.html",
    "href": "posts/MedicalBiasJLS/DataCleaning.html",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder"
  },
  {
    "objectID": "posts/MedicalBiasJLS/DataCleaning.html#read-in-data",
    "href": "posts/MedicalBiasJLS/DataCleaning.html#read-in-data",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Read in data",
    "text": "Read in data\n\nallergies = pd.read_csv('allergies.csv')\ncareplans = pd.read_csv('careplans.csv')\nconditions = pd.read_csv('conditions.csv')\nencounters = pd.read_csv('encounters.csv') ##NOT USING RN, DO WE NEED?\nimmunizations = pd.read_csv('immunizations.csv')\nmedications = pd.read_csv('medications.csv')\nobservations = pd.read_csv('observations.csv')\npatients = pd.read_csv('patients.csv')\nprocedures = pd.read_csv('procedures.csv')\n\n\nlen(patients[patients['ethnicity'] == 'scottish']['ethnicity'])\n\n27"
  },
  {
    "objectID": "posts/MedicalBiasJLS/DataCleaning.html#clean-up-dataframes-have-one-row-per-patient",
    "href": "posts/MedicalBiasJLS/DataCleaning.html#clean-up-dataframes-have-one-row-per-patient",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Clean up dataframes: have one row per patient",
    "text": "Clean up dataframes: have one row per patient\n\n## ALLERGIES\nallergies_pivot = pd.get_dummies(allergies['DESCRIPTION'])\nallergies_pivot['PATIENT'] = allergies['PATIENT']\nallergies_pivot = allergies_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## CAREPLANS\ncareplans_pivot = pd.get_dummies(careplans['DESCRIPTION'])\ncareplans_pivot['PATIENT'] = careplans['PATIENT']\ncareplans_pivot = careplans_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## CONDITIONS\nconditions_pivot = pd.get_dummies(conditions['DESCRIPTION'])\nconditions_pivot['PATIENT'] = conditions['PATIENT']\nconditions_pivot = conditions_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## IMMUNIZATIONS\nimmunizations_pivot = pd.get_dummies(immunizations['DESCRIPTION'])\nimmunizations_pivot['PATIENT'] = immunizations['PATIENT']\nimmunizations_pivot = immunizations_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## MEDICATIONS\nmedications_pivot = pd.get_dummies(medications['DESCRIPTION'])\nmedications_pivot['PATIENT'] = medications['PATIENT']\nmedications_pivot = medications_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## OBSERVATIONS\nobservations['VALUE'] = pd.to_numeric(observations['VALUE'], errors='coerce')\n\n# Pivot table with mean aggregation\nobservations_pivot= observations.pivot_table(index=observations.index, columns='DESCRIPTION', values='VALUE', fill_value=0, aggfunc='mean')\nobservations_pivot['PATIENT'] = observations['PATIENT']\nobservations_pivot = observations_pivot.groupby('PATIENT').sum().reset_index()\n\nobservations_pivot.to_csv('observations_pivot.csv', index=False)\n\n\n## PROCEDURES\nprocedures_pivot = pd.get_dummies(procedures['DESCRIPTION'])\nprocedures_pivot['PATIENT'] = procedures['PATIENT']\nprocedures_pivot = procedures_pivot.groupby('PATIENT').sum().reset_index()\n\n\nREMOVE MA US from Patient info (they are all from mass)\n\n#import re\nplaces = patients['birthplace']\ncleaned_places = [place.replace(\" MA US\", \"\") if \" MA US\" in place else place for place in places]\n\n\npatients['birthplace'] = cleaned_places\n\n\ntown_names = patients['birthplace'].unique().tolist()\n#some towns do not appear in birthplace but appear in current addresses, I modified the function below to print these towns and create this list, then\n# reverted to the function so it wouldn't have a long output\ntowns_not_in_bp = ['Sandwich', 'Uxbridge', 'Holland', 'Monson', 'Wendell', 'Wayland', 'Rochester', 'Belchertown', 'Lanesborough', 'Hatfield',\n                  'Georgetown', 'Lakeville', 'Princeton', 'Blackstone', 'Hinsdale', 'Harvard', 'Chesterfield', 'Wellfleet', 'Northfield', 'Hubbardston',\n                  'Windsor', 'Wales', 'Sandisfield', 'Bolton', 'Truro', 'Southwick', 'Sheffield', 'Scituate', 'Halifax', 'Nahant', 'Stockbridge', 'Berlin']\n                \nfor town in towns_not_in_bp:\n    town_names.append(town)\n\n\ndef extract_town_name(address, town_list):\n    # Check if any part of the address matches any town name in the list\n    for town in town_list:\n        if town in address:\n            return town\n    print(address)\n    return None\n\npatients['curr_town'] = \"\"\n\nfor index, address in enumerate(patients['address']):\n    # Extract the town name\n    town_name = extract_town_name(address, town_names)\n    \n    # Assign the extracted town name to the corresponding entry in the 'curr_town' column\n    patients.at[index, 'curr_town'] = town_name\n\n\npatients.to_csv('patient_clean.csv', index=False)"
  },
  {
    "objectID": "posts/MedicalBiasJLS/DataCleaning.html#add-suffixes-to-columns",
    "href": "posts/MedicalBiasJLS/DataCleaning.html#add-suffixes-to-columns",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Add suffixes to columns",
    "text": "Add suffixes to columns\n\npatients = patients.rename(columns={'patient': 'PATIENT'})\n\ndef add_suffix(df, suffix):\n    renamed_columns = {}\n    for col_name in df.columns:\n        if col_name != 'PATIENT':\n            renamed_columns[col_name] = col_name + '_' + suffix\n        else:\n            renamed_columns[col_name] = col_name\n    return df.rename(columns=renamed_columns)\n\nallergies_clean = add_suffix(allergies_pivot, 'ALLERGIES')\ncareplans_clean = add_suffix(careplans_pivot, 'CAREPLANS')\nconditions_clean = add_suffix(conditions_pivot, 'CONDITIONS')\nimmunizations_clean = add_suffix(immunizations_pivot, 'IMMUNIZATIONS')\nmedications_clean = add_suffix(medications_pivot, 'MEDICATIONS')\nobservations_pivot = add_suffix(observations_pivot, 'OBSERVATIONS')\nprocedures_clean = add_suffix(procedures_pivot, 'PROCEDURES')"
  },
  {
    "objectID": "posts/MedicalBiasJLS/DataCleaning.html#prep-data",
    "href": "posts/MedicalBiasJLS/DataCleaning.html#prep-data",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Prep Data",
    "text": "Prep Data\n\nle = LabelEncoder()\n\n# our data-prepping function for modeling\ndef prep_data(patients, conditions, illness_descriptions, observations):\n\n    # make patients column match others for merging, drop unnecessary information and NA vals\n    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n    patients = patients.drop(columns=['birthdate', 'marital','deathdate','ssn', 'address', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n    patients = patients.dropna()\n    conditions = conditions.dropna()\n\n    # merge datasets (patient info and corresponding conditions)\n    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n\n    # create y\n    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n    merged_df = merged_df.drop(columns=illness_descriptions)\n    \n    # return split x, y, and all of the code tracking dicts\n    return merged_df"
  },
  {
    "objectID": "posts/MedicalBiasJLS/DataCleaning.html#merge-datasets",
    "href": "posts/MedicalBiasJLS/DataCleaning.html#merge-datasets",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Merge datasets",
    "text": "Merge datasets\nSeparate conditions dataframe based on disease group\n\n## DIABETES\nillness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n# dataset to be used for analysis\nsubset_conditions.to_csv('conditions_diabetes.csv', index=False)\n\n\n## PREGNANCY\nillness_descriptions = ['PATIENT','Miscarriage in first trimester_CONDITIONS','Miscarriage in second trimester_CONDITIONS',\n                        'Complication occuring during pregnancy_CONDITIONS','Preeclampsia_CONDITIONS', 'Antepartum eclampsia_CONDITIONS',\n                        'Tubal pregnancy_CONDITIONS', 'Congenital uterine anomaly_CONDITIONS', 'Blighted ovum_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n# dataset to be used for analysis\nsubset_conditions.to_csv('conditions_pregnancy.csv', index=False)\n\n\n## CANCER\nillness_descriptions = ['PATIENT','Non-small cell lung cancer (disorder)_CONDITIONS', 'Non-small cell carcinoma of lung  TNM stage 4 (disorder)_CONDITIONS',\n                        'Primary small cell malignant neoplasm of lung  TNM stage 4 (disorder)_CONDITIONS','Non-small cell carcinoma of lung  TNM stage 2 (disorder)_CONDITIONS',\n                        'Non-small cell lung cancer (disorder)_CONDITIONS', 'Suspected lung cancer (situation)_CONDITIONS', 'Malignant tumor of colon_CONDITIONS',\n                        'Overlapping malignant neoplasm of colon_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n\nsubset_conditions.to_csv('conditions_cancer.csv', index=False)\n\n\n## HEART\nillness_descriptions = ['PATIENT','Coronary Heart Disease_CONDITIONS', 'History of cardiac arrest (situation)_CONDITIONS', 'Cardiac Arrest_CONDITIONS',\n                        'History of myocardial infarction (situation)_CONDITIONS', 'Myocardial Infarction_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\nsubset_conditions.to_csv('conditions_heart.csv', index=False)\n\n\n## LUNGS\nillness_descriptions = ['PATIENT','Asthma_CONDITIONS', 'Pulmonary emphysema (disorder)_CONDITIONS', 'Seasonal allergic rhinitis_CONDITIONS', \n                        'Acute bronchitis (disorder)_CONDITIONS', 'Chronic obstructive bronchitis (disorder)_CONDITIONS',\n                        'Childhood asthma_CONDITIONS', 'Perennial allergic rhinitis with seasonal variation_CONDITIONS',\n                        'Perennial allergic rhinitis_CONDITIONS', 'Acute bacterial sinusitis (disorder)_CONDITIONS', 'Chronic sinusitis (disorder)_CONDITIONS',\n                        'Sinusitis (disorder)_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\nsubset_conditions.to_csv('conditions_lungs.csv', index=False)"
  },
  {
    "objectID": "posts/EndCourseEvals/FinalCourseEvals.html",
    "href": "posts/EndCourseEvals/FinalCourseEvals.html",
    "title": "CSCI 0451: End-Of-Course Reflection",
    "section": "",
    "text": "Julia Joy\n\n\n\n\n\nHow often did you attend class? (e.g. “almost always,” “I missed three times,” etc.) I attended nearly every class, with the exception of I believe 2 classes.\nHow often did you take notes on the core readings ahead of the class period? I always read the core readings ahead of class, probably taking notes on them around half of the time.\nHow often were you prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? Since I completed nearly every warmup, I would say I was almost always ready to present the daily warm-up exercises, probably with the exception of 2 times.\nHow many times did you actually present the daily warm-up to your team? This estimate is a bit tricky for me to remember, but I would guess around 8 times.\nHow many times did you ask your team for help while presenting the daily warm-up? I think I directly asked for help once.\nHow often did you learn something new from a teammate’s presentation of the daily warm-up? I feel like I can frequently learn a little something new, even if I did the warmup correctly it is always interesting to see my other groupmates approaches or perspectives, so likely once a week I learned some kind of new little thing.\nHow often did you help a teammate during the daily warm-up presentation? Our group was great at contributing together, with a direct help I probably helped twice.\nDid you contribute a question for our guest speaker? I don’t believe I contributed a question for our guest speaker.\n\n\n\n\n\nHow often did you attend Student Hours or Peer Help? I believe I attended student hours/peer help once or twice.\nHow often did you ask for or receive help from your fellow students? We often met together in a study group, so I probably ask someone a question around once a week whether its about the warmups, blog posts, or general clarification of things were supposed to be doing.\nDid you regularly participate in a study group outside class? Yes.\nHow often did you post questions or answers in Slack? I think I have maybe posted one question.\n\n\n\n\n\nHow many blog posts did you submit? Five.\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: 5\nRevisions useful: 0\nRevisions encouraged: 0\nIncomplete: 0\n\nRoughly how many hours per week did you spend on this course outside of class? I would say 9-11 throughout the semester, and that number shooting up to 12-13 once we were working on the final project.\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what did you do in order to pursue your interest?\nI expressed an interest in focusing a bit extra on the implementation category. I always feel like more practice with effective coding and learning how to use new tools is so invaluable, and I believe I have definitely successfully focused in on this category throughout both my blog post work as well as my final project work. My perceptron and linear regression blog posts honed in on the implementation category as I effectively implemented machine learning algorithms/models. Through my final project, I performed various implementation of machine learning models, such as decision trees and random forests, the models were implemented with and trained on synthetic medical data, to perform our final projects goals of predicting health risks by outputting model-calculated risk scores. Embracing these assignments as learning opportunities, I believe I successfully refined my skills and proficiency in the realm of Implementation of machine learning.\nIn my midcourse evals, I said that “Our third week in the semester too maybe me more interested in the social responsibility side of machine learning algorithms, so potentially it would be interesting to conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm for my final project? We shall see!” Looking back, I would say that I definitely focused in on the social responsibility category as well! I chose blog posts and projects that interested me throughout, so it is pleasant to see that I do find myself having a consistent pattern of interest in the social responsibility category. I completed the replication study and the WiDS conference blog posts both pertaining to the social responsibility category. Through my final project, I communicating our results in the context of identifying potential biases across different groups such as factors based on race, ethnicity, birthplace, and current town of residence, which required me to learn how to identify, understand, and evaluate environmental and societal inequities that may be influencing real-world health outcomes.\nI feel happy and proud of my work as it pertains to focusing in on any categories. I successfully focused in on Implementation, the category I set a goal out to do, and I discovered a newfound interest in the category of Social Responsibility as I noticed a recurring theme in the interest of my work.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways did you meet your goals from the beginning of the course? Be specific: explain what the goal was and what you did to meet it.\nIn what ways did you not meet your goals from the beginning of the course? Be specific: explain what the goal was and what the gap was between what you aspired to and what happened.\nIf there’s any context you want to share about how you fared relative to your goals, please do!\n\n\n\nIn my beginning as well as my mid-semester course reflection, I aimed to complete “5-6 solid good work blog posts”. I believe I have successfully completed this goal, as I have completed 5 blog posts to the E-excellent level, with my final project blog post brings it to 6 completed posts. In my goals for the beginning, I even said I would “leave the space for 1-2 blog posts to remain in the M category”, so I feel happy and successful that I actually superseeded my goal and had only E blog posts.\n\n\n\nOverall, I acted in accordance with the course presence/participation goals I set. I believe I only missed two classes this semester, which completes my goal to attend nearly every class unless truly having to miss it. In my goal setting I had aimed to attempt every warmup before class and I can happily and proudly say that I have done so for every class period I attended. I have also stuck in line of my goals of doing most of the course readings as well as thoroughly participating in our small warmup groups. As before in my mid-semester reflection, I was right in my guessing that I would not be the biggest slack user, but I have asked all the questions I needed answered in some form or another, whether it be study group or an office hour drop in. I did struggle, as I expected, to raise my hand to answer questions in class, my shyness outweighing everything else. It is something I have tried to work on for a long time, so I had a continued awareness for it, but not much improvement in that regard. I believe I met all of the goals I set for presence and participation in this course.\n\n\n\nIn my original goal setting, I said that I was “excited about this project as I feel like it will allow me to explore what machine learning can be in an area that I’m a bit more interested in, whatever that may be”, and that “I would like to complete all project milestones on time to keep myself and the group accountable in our semester time frame, contribute to the project in an equal manner including in the algorithm implementation and project presentation, and do what I can to be a great group project partner”. At the time I did not have an idea of what I exactly would want to work on in my final project, but during the mid-course reflections I mentioned that I would be interested in studying a data set and/or experimenting with assessing algorithmic bias. Through the project pitches, I found a project that greatly interested me: Implementing various machine learning models to generate risk scores and interpret its results in order to identify potential real-world inequities reflected by model generated risk scores. I worked with Lindsey and Sophie, and I found us to be a very well-oiled, effective and collaborative team. We did indeed complete all project milestones on time, keeping ourselves accountable in our semester time frame. We worked on the project bit by bit, piece by piece, and I was impressed myself with how well we timed the completion of our project. We all contributed to the project in an equal manner including in the algorithm implementation and project presentation, which was reflected in our group contribution statements on the project blog post. I was happy with my work in data visualization, model implementation, model experimentation, result analysis, project cleanup, and presentation. I think our project was successful both in its content and in our work as a team, more than meeting my expectations and goals for our project.\n\n\n\nIs there anything else that you want to share with me about what you learned, how you participated, or what you achieved in CSCI 0451?\nThere is nothing much else I would like to share, except that I am proud to look back and reflect on all the work and effort I have put into Machine Learning this semester."
  },
  {
    "objectID": "posts/EndCourseEvals/FinalCourseEvals.html#the-data",
    "href": "posts/EndCourseEvals/FinalCourseEvals.html#the-data",
    "title": "CSCI 0451: End-Of-Course Reflection",
    "section": "",
    "text": "How often did you attend class? (e.g. “almost always,” “I missed three times,” etc.) I attended nearly every class, with the exception of I believe 2 classes.\nHow often did you take notes on the core readings ahead of the class period? I always read the core readings ahead of class, probably taking notes on them around half of the time.\nHow often were you prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? Since I completed nearly every warmup, I would say I was almost always ready to present the daily warm-up exercises, probably with the exception of 2 times.\nHow many times did you actually present the daily warm-up to your team? This estimate is a bit tricky for me to remember, but I would guess around 8 times.\nHow many times did you ask your team for help while presenting the daily warm-up? I think I directly asked for help once.\nHow often did you learn something new from a teammate’s presentation of the daily warm-up? I feel like I can frequently learn a little something new, even if I did the warmup correctly it is always interesting to see my other groupmates approaches or perspectives, so likely once a week I learned some kind of new little thing.\nHow often did you help a teammate during the daily warm-up presentation? Our group was great at contributing together, with a direct help I probably helped twice.\nDid you contribute a question for our guest speaker? I don’t believe I contributed a question for our guest speaker.\n\n\n\n\n\nHow often did you attend Student Hours or Peer Help? I believe I attended student hours/peer help once or twice.\nHow often did you ask for or receive help from your fellow students? We often met together in a study group, so I probably ask someone a question around once a week whether its about the warmups, blog posts, or general clarification of things were supposed to be doing.\nDid you regularly participate in a study group outside class? Yes.\nHow often did you post questions or answers in Slack? I think I have maybe posted one question.\n\n\n\n\n\nHow many blog posts did you submit? Five.\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: 5\nRevisions useful: 0\nRevisions encouraged: 0\nIncomplete: 0\n\nRoughly how many hours per week did you spend on this course outside of class? I would say 9-11 throughout the semester, and that number shooting up to 12-13 once we were working on the final project."
  },
  {
    "objectID": "posts/EndCourseEvals/FinalCourseEvals.html#what-you-learned",
    "href": "posts/EndCourseEvals/FinalCourseEvals.html#what-you-learned",
    "title": "CSCI 0451: End-Of-Course Reflection",
    "section": "",
    "text": "At the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what did you do in order to pursue your interest?\nI expressed an interest in focusing a bit extra on the implementation category. I always feel like more practice with effective coding and learning how to use new tools is so invaluable, and I believe I have definitely successfully focused in on this category throughout both my blog post work as well as my final project work. My perceptron and linear regression blog posts honed in on the implementation category as I effectively implemented machine learning algorithms/models. Through my final project, I performed various implementation of machine learning models, such as decision trees and random forests, the models were implemented with and trained on synthetic medical data, to perform our final projects goals of predicting health risks by outputting model-calculated risk scores. Embracing these assignments as learning opportunities, I believe I successfully refined my skills and proficiency in the realm of Implementation of machine learning.\nIn my midcourse evals, I said that “Our third week in the semester too maybe me more interested in the social responsibility side of machine learning algorithms, so potentially it would be interesting to conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm for my final project? We shall see!” Looking back, I would say that I definitely focused in on the social responsibility category as well! I chose blog posts and projects that interested me throughout, so it is pleasant to see that I do find myself having a consistent pattern of interest in the social responsibility category. I completed the replication study and the WiDS conference blog posts both pertaining to the social responsibility category. Through my final project, I communicating our results in the context of identifying potential biases across different groups such as factors based on race, ethnicity, birthplace, and current town of residence, which required me to learn how to identify, understand, and evaluate environmental and societal inequities that may be influencing real-world health outcomes.\nI feel happy and proud of my work as it pertains to focusing in on any categories. I successfully focused in on Implementation, the category I set a goal out to do, and I discovered a newfound interest in the category of Social Responsibility as I noticed a recurring theme in the interest of my work."
  },
  {
    "objectID": "posts/EndCourseEvals/FinalCourseEvals.html#reflecting-on-goals",
    "href": "posts/EndCourseEvals/FinalCourseEvals.html#reflecting-on-goals",
    "title": "CSCI 0451: End-Of-Course Reflection",
    "section": "",
    "text": "For each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways did you meet your goals from the beginning of the course? Be specific: explain what the goal was and what you did to meet it.\nIn what ways did you not meet your goals from the beginning of the course? Be specific: explain what the goal was and what the gap was between what you aspired to and what happened.\nIf there’s any context you want to share about how you fared relative to your goals, please do!\n\n\n\nIn my beginning as well as my mid-semester course reflection, I aimed to complete “5-6 solid good work blog posts”. I believe I have successfully completed this goal, as I have completed 5 blog posts to the E-excellent level, with my final project blog post brings it to 6 completed posts. In my goals for the beginning, I even said I would “leave the space for 1-2 blog posts to remain in the M category”, so I feel happy and successful that I actually superseeded my goal and had only E blog posts.\n\n\n\nOverall, I acted in accordance with the course presence/participation goals I set. I believe I only missed two classes this semester, which completes my goal to attend nearly every class unless truly having to miss it. In my goal setting I had aimed to attempt every warmup before class and I can happily and proudly say that I have done so for every class period I attended. I have also stuck in line of my goals of doing most of the course readings as well as thoroughly participating in our small warmup groups. As before in my mid-semester reflection, I was right in my guessing that I would not be the biggest slack user, but I have asked all the questions I needed answered in some form or another, whether it be study group or an office hour drop in. I did struggle, as I expected, to raise my hand to answer questions in class, my shyness outweighing everything else. It is something I have tried to work on for a long time, so I had a continued awareness for it, but not much improvement in that regard. I believe I met all of the goals I set for presence and participation in this course.\n\n\n\nIn my original goal setting, I said that I was “excited about this project as I feel like it will allow me to explore what machine learning can be in an area that I’m a bit more interested in, whatever that may be”, and that “I would like to complete all project milestones on time to keep myself and the group accountable in our semester time frame, contribute to the project in an equal manner including in the algorithm implementation and project presentation, and do what I can to be a great group project partner”. At the time I did not have an idea of what I exactly would want to work on in my final project, but during the mid-course reflections I mentioned that I would be interested in studying a data set and/or experimenting with assessing algorithmic bias. Through the project pitches, I found a project that greatly interested me: Implementing various machine learning models to generate risk scores and interpret its results in order to identify potential real-world inequities reflected by model generated risk scores. I worked with Lindsey and Sophie, and I found us to be a very well-oiled, effective and collaborative team. We did indeed complete all project milestones on time, keeping ourselves accountable in our semester time frame. We worked on the project bit by bit, piece by piece, and I was impressed myself with how well we timed the completion of our project. We all contributed to the project in an equal manner including in the algorithm implementation and project presentation, which was reflected in our group contribution statements on the project blog post. I was happy with my work in data visualization, model implementation, model experimentation, result analysis, project cleanup, and presentation. I think our project was successful both in its content and in our work as a team, more than meeting my expectations and goals for our project.\n\n\n\nIs there anything else that you want to share with me about what you learned, how you participated, or what you achieved in CSCI 0451?\nThere is nothing much else I would like to share, except that I am proud to look back and reflect on all the work and effort I have put into Machine Learning this semester."
  },
  {
    "objectID": "posts/Perceptron/index.html",
    "href": "posts/Perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "In this blog post, I implemented the perceptron algorithm, a fundamental part of many machine learning models. Through a series of visualizations, I demonstrate the behavior of the perceptron algorithm on both linearly separable and non-linearly separable datasets in two dimensions, as well as multi-dimensional feature spaces. Additionally, I implemented the minibatch perceptron, exploring training efficiency compared to the regular perceptron as well as looking at loss convergence behavior. Through this process, I learned the ins and outs of the perceptron algorithm, and how it responds to different types of data.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Perceptron/index.html#part-a-perceptron-implementation",
    "href": "posts/Perceptron/index.html#part-a-perceptron-implementation",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Part A: Perceptron Implementation",
    "text": "Part A: Perceptron Implementation\nThe implementation of the perceptron algorithm itself is located in file perceptron.py. In this file I implemented the score, predict, loss, step, and grad functions:\nscore: Computes the scores for each data point in the feature matrix X. The formula for the ith entry of s is s[i] = &lt;self.w, x[i]&gt;, the cross product of the input data and the model weights.\npredict: Calculates y_hat, the predictions for each data point in the feature matrix X. y_hat takes on values of either 1 or 0, where y_hat is assigned 1 if the score &gt;=0 , and y_hat is assigned 0 otherwise.\nloss: Computes the misclassification rate of the model by calculating the mean of all the misclassified data points.\nstep: Compute one update step of the perceptron update using the feature matrix X and target vector y.\ngrad: This function computes the gradient of the empirical risk. If a data point is correctly classified, it returns a gradient of 0. If a data point is misclassified, it returns the gradient of the cross product between the input data and the output.\nIn order to check if this code is correctly working, I must perform experiments on different kinds of data. ## Part B: Experiments\n\nLinearly Separable Data:\nI first visualize the data, to see how the data I am working with is indeed linearly-separable. I run the minimal training loop code from class notes, aiming to end with loss = 0 on linearly separable data.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(12345)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nWe can clearly see that the data could be separated by a line, dividing the two classes, orange and blue.\nI then run the minimal training loop, and plot the evolution of the loss function.\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss = 1.0\nlossVec = []\n\nn = X.size()[0]\n\n#terminates only if linearly-separable\nwhile loss &gt; 0: \n    loss = p.loss(X, y) \n    lossVec.append(loss)\n    \n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    opt.step(x_i, y_i)\n\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 6))\nplt.plot(lossVec, color = \"slategrey\")\nplt.scatter(torch.arange(len(lossVec)), lossVec, color = \"slategrey\", s=3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(f\"Evolution of the Loss Function with Linearly Separable Data\")\n#check final loss value as a means of verifying model \nfinalLossVal = lossVec[len(lossVec)-1]\n\n\n\n\n\n\n\n\nWe can see that the loss decreases after many iterations and eventually reaches 0 when running on this linearly-separable data."
  },
  {
    "objectID": "posts/Perceptron/index.html#non-linearly-separable-data",
    "href": "posts/Perceptron/index.html#non-linearly-separable-data",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Non-Linearly separable Data:",
    "text": "Non-Linearly separable Data:\nWe must first generate some data that we know will not be linearly separable. We can do this by editing code provided by the class notes. Non-linearly separable data means that certain data points are in the space that would normally be considered distinctly one class or another, these points are our overlapping data points.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef overlapPerceptronData(n_points = 300, noise = 0.2, p_dims = 2, numOverlap = 30):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # now generate overlapping points\n    overlappingX = torch.rand(numOverlap, p_dims) * (X.max() - X.min()) + X.min()\n    # add in the overlapping points to the original data points \n    overlappingX = torch.cat((overlappingX, torch.ones((numOverlap, 1))), 1)\n    # generate labels for these new points \n    overlappingy = torch.ones(numOverlap, dtype=torch.bool)\n    X = torch.cat((X, overlappingX), dim=0)\n    y = torch.cat((y, overlappingy), dim=0)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nIn order to visualize the now newly-altered, non-linearly separable data, we can plot the data as a scatterplot, also using the class notes.\n\ndef plotOverlap(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = overlapPerceptronData()\nnonLinPlot = plotOverlap(X, y, ax)\n\n\n\n\n\n\n\n\nWe can see that the overlapping of the data was successful, thus meaning that our data is no longer linearly-separable. Now we can rerun the perceptron algorithm, but since the data is not linearly separable it must be run for a certain number of iterations to ensure that the algorithm terminates. The number of iterations is set to 1000, plenty for the algorithm to run its due course.\n\ntorch.manual_seed(1234)\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\nX, y = overlapPerceptronData()\n\nlossVecNLS = []\n\nfor index in range(0, 1000):\n    \n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = opt.step(x_i, y_i)\n    loss = p.loss(X, y).item()\n    lossVecNLS.append(loss)\n\nfinalWeights = opt.model.w\n\nNow we visualize the evolution of the loss function for a non-linearly separable dataset. We can note that although the loss does decrease over iterations, it never fully reaches 0, unlike the plot of the evolution of the loss function with linearly-separable data.\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 6))\nplt.plot(lossVecNLS, color = \"slategrey\")\nplt.scatter(torch.arange(len(lossVecNLS)), lossVecNLS, color = \"slategrey\", s=3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(f\"Evolution of the Loss Function with Non-Linearly Separable Data\")\n#check final loss value as a means of verifying model \nfinalLossVal = lossVecNLS[len(lossVecNLS)-1]\n\n\n\n\n\n\n\n\nLet’s show the decision boundary in the final iteration, after 1000 runs of the perceptron updates. We can utilize the draw_line function from lecture to do so.\n\n#function from lecture\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs) \n    fig, ax = plt.subplots(1, 1)\n\nfig, ax = plt.subplots(1, 1)\nplotOverlap(X, y, ax)\ndraw_line(finalWeights, x_min = -2, x_max = 3, ax = ax, color = \"black\", linestyle = \"dashed\")"
  },
  {
    "objectID": "posts/Perceptron/index.html#data-in-more-than-two-dimensions",
    "href": "posts/Perceptron/index.html#data-in-more-than-two-dimensions",
    "title": "Implementing the Perceptron Algorithm",
    "section": "Data in More Than Two Dimensions:",
    "text": "Data in More Than Two Dimensions:\nThe perceptron algorithm is also able to work on data with more than 2 dimensions, in this case the generated data has 5 dimensions. Similarly to how I did before, I rerun the perceptron algorithm, and visualize the evolution of the loss function.\n\nX, y = overlapPerceptronData(n_points = 300, noise = 0.2, p_dims = 5, numOverlap =0)\n\ntorch.manual_seed(12345)\n\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\nX, y = overlapPerceptronData()\nlossVecMD = []\n\nfor index in range(0, 1000):\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = opt.step(x_i, y_i)\n\n    loss = p.loss(X, y).item()\n    lossVecMD.append(loss)\n\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 6))\n\nplt.plot(lossVecMD, color = \"slategrey\")\nplt.scatter(torch.arange(len(lossVecMD)), lossVecMD, color = \"slategrey\", s=3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(f\"Evolution of the Loss Function with Multiple Dimensions\")\n#check final loss value as a means of verifying model \nfinalLossVal = lossVecMD[len(lossVecMD)-1]\nfinalLossVal\n\n0.05454545468091965\n\n\n\n\n\n\n\n\n\nWe can see that the loss does decrease over the iterations ran, however since the score does not fully reach zero, I believe that the data is not linearly-separable.\n\nPart C: Minibatch Perceptron\nIn order to implement mini-batch updating, the perceptron.grad() method needs to be updated to accept a submatrix (of size k * p) of the feature matrix X. After updating, we can proceed.\n\n\nk = 1\n\nX, y = perceptron_data()\n\ntorch.manual_seed(12345) \np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss = 1.0\nlossVecMinibatch = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    loss = p.loss(X, y) \n    lossVecMinibatch.append(loss)\n    \n    k = 1 \n    ix = torch.randperm(X.size(0))[:k]\n\n    xSubmatrix = X[ix]\n    ySubmatrix = y[ix]\n\n    # opt step with the random submatrix\n    opt.step(xSubmatrix, ySubmatrix)\n\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 6))\n\nplt.plot(lossVecMinibatch, color = \"slategrey\")\nplt.scatter(torch.arange(len(lossVecMinibatch)), lossVecMinibatch, color = \"slategrey\", s=3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(f\"Evolution of the Loss Function with Multiple Dimensions\")\n#check final loss value as a means of verifying model \nfinalLossVal = lossVecMinibatch[len(lossVecMinibatch)-1]\nfinalLossVal\n\ntensor(0.)\n\n\n\n\n\n\n\n\n\nWhen k = 1, we can see that the mini-batch perceptron performs similarly to the regular perceptron on linearly separable data, as its loss eventually reaches 0.\n\nX, y = overlapPerceptronData()\n\ntorch.manual_seed(12345) \np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss = 1.0\nlossVecMinibatch = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    loss = p.loss(X, y) \n    lossVecMinibatch.append(loss)\n    \n    k = 1 \n    ix = torch.randperm(X.size(0))[:k]\n\n    xSubmatrix = X[ix]\n    ySubmatrix = y[ix]\n\n    # opt step with the random submatrix\n    opt.step(xSubmatrix, ySubmatrix)\n\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 6))\n\nplt.plot(lossVecMinibatch, color = \"slategrey\")\nplt.scatter(torch.arange(len(lossVecMinibatch)), lossVecMinibatch, color = \"slategrey\", s=3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(f\"Evolution of the Loss Function with Multiple Dimensions\")\n#check final loss value as a means of verifying model \nfinalLossVal = lossVecMinibatch[len(lossVecMinibatch)-1]\nfinalLossVal\n\ntensor(0.0455)\n\n\n\n\n\n\n\n\n\nWith non linearly-separable data, the loss does not ever fully reach 0, once again performing similarly to the regular perceptron. Thus we see that when k = 1, minibatch perceptron performs similarly to regular perceptron.\n\n\nK = 10\n\nX, y = perceptron_data() \n\ntorch.manual_seed(12345) \np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss = 1.0\nlossVecMinibatch = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    loss = p.loss(X, y) \n    lossVecMinibatch.append(loss)\n    \n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n\n    xSubmatrix = X[ix]\n    ySubmatrix = y[ix]\n\n    # opt step with the random submatrix\n    opt.step(xSubmatrix, ySubmatrix)\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 6))\n\nplt.plot(lossVecMinibatch, color = \"slategrey\")\nplt.scatter(torch.arange(len(lossVecMinibatch)), lossVecMinibatch, color = \"slategrey\", s=3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(f\"Evolution of the Loss Function with Multiple Dimensions\")\n#check final loss value as a means of verifying model \nfinalLossVal = lossVecMinibatch[len(lossVecMinibatch)-1]\nfinalLossVal\n\ntensor(0.)\n\n\n\n\n\n\n\n\n\nWhen k = 10, we can see that minibatch perceptron can still find a separating line in 2d, as its loss eventually reaches 0.\n\n\nK = n\n\nX, y = overlapPerceptronData() \n\ntorch.manual_seed(12345) \np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss = 1.0\nlossVecMinibatch = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n\n    loss = p.loss(X, y)\n    lossVecMinibatch.append(loss)\n    \n    k = n\n    ix = torch.randperm(X.size(0))[:k]\n\n    xSubmatrix = X[ix]\n    ySubmatrix = y[ix]\n\n    # opt step with the random submatrix\n    opt.step(xSubmatrix, ySubmatrix)\n\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.figure(figsize=(10, 6))\n\nplt.plot(lossVecMinibatch, color = \"slategrey\")\nplt.scatter(torch.arange(len(lossVecMinibatch)), lossVecMinibatch, color = \"slategrey\", s=3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(f\"Evolution of the Loss Function with Multiple Dimensions\")\n#check final loss value as a means of verifying model \nfinalLossVal = lossVecMinibatch[len(lossVecMinibatch)-1]\nfinalLossVal\n\ntensor(0.0576)\n\n\n\n\n\n\n\n\n\nWhen k = n, meaning that the batch size the size of the entire data set, the minibatch perceptron can converge even when the data is not linearly separable, very nearly reaching a loss of 0.\n\n\nPart D: Writing\nWhat is the runtime complexity of a single iteration of the perceptron algorithm? Does the runtime complexity of a single iteration depend on the number of data points? What about the number of features? If you implemented minibatch perceptron, what is the runtime complexity of a single iteration of the minibatch perceptron algorithm?\nThe runtime complexity of a single iteration of the perceptron algorithm depends on the number of features (p). For each data point (represented by a row in feature matrix X), the perceptron algorithm computes the dot product the weight vector w and a row of the feature matrix X. Thus the runtime complexity of a single iteration is O(p).\nWith the minibatch perceptron, the runtime complexity of a single iteration is dependent on the size of the minibatch (k). The minibatch perceptron algorithm computes the dot product of each data point in the minibatch (rather than 1 single data point like in the typical perceptron algorithm). Thus the runtime complexity of a single iteration of the minibatch perceptron algorithm is O(kp).\n\n\nSummary Conclusion\nThrough this exploration of the Perceptron algorithm, I have learned its behavior in functionality and response to different types of data. Experimenting with different types of data revealed the perceptron’s ability to learn separating boundaries. Visualizations of evolution of the loss function allowed for easier understanding of how the algorithm itself operated, as well as revealing the differences between the behaviors of the perceptron algorithm on different types of data. I was ultimately able to successfully implement and use the perceptron algorithm for linearly separable data, non-linearly separable data, and data with more than 2 dimensions, as well as implement a version of the perceptron algorithm, the minibatch perceptron. This blog post serving as a guide to understanding the Perceptron Algorithm."
  },
  {
    "objectID": "posts/Decision Making/index.html",
    "href": "posts/Decision Making/index.html",
    "title": "Optimal Decision Making",
    "section": "",
    "text": "import pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n/var/folders/6v/npbkg9_919s0m2j5qd702gf00000gn/T/ipykernel_10000/516695261.py:1: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n#PART B Visualizations \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#figure 1 \nplt.figure(figsize=(10, 6))\nsns.countplot(x='person_home_ownership', hue='loan_intent', data=df_train)\nplt.title('Loan Intent vs. Home Ownership Status')\nplt.xlabel('Home Ownership')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.legend(title='Loan Intent')\nplt.show()\n#figure 2\nageCategory = [20, 30, 40, 50, 60, 70, 80, 90, 100]\ndf_train['ageCategory'] = pd.cut(df_train['person_age'], bins=ageCategory)\nIR = df_train.groupby(['ageCategory']).agg({'loan_int_rate': 'mean'}).reset_index()\nIR = IR.sort_values(by='ageCategory', ascending=True)\nprint(IR)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='ageCategory', y='loan_int_rate', data=IR)\nplt.title('Mean Interest Rate by Age Category')\nplt.xlabel('Age Category')\nplt.ylabel('Mean Interest Rate')\nplt.ylim(7, 11.5) \nplt.show()\n\n/var/folders/6v/npbkg9_919s0m2j5qd702gf00000gn/T/ipykernel_10000/3508508793.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  IR = df_train.groupby(['ageCategory']).agg({'loan_int_rate': 'mean'}).reset_index()\n\n\n  ageCategory  loan_int_rate\n0    (20, 30]      10.997363\n1    (30, 40]      11.053365\n2    (40, 50]      11.021150\n3    (50, 60]      11.054049\n4    (60, 70]      11.019730\n5    (70, 80]       9.636000\n6    (80, 90]       7.510000\n7   (90, 100]            NaN\n#summary table\ndfIRSorted = df_train.dropna(subset=['loan_int_rate'])\ndfIRSorted = df_train.sort_values(by='loan_int_rate', ascending=True)\n\nlowest10IR = dfIRSorted[['ageCategory', 'person_home_ownership', 'loan_int_rate']]\nlowest10IR = lowest10IR.head(10).drop_duplicates()\n\nprint(lowest10IR)\n\n      ageCategory person_home_ownership  loan_int_rate\n6061     (20, 30]              MORTGAGE           5.42\n24160    (30, 40]              MORTGAGE           5.42\n15162    (20, 30]                  RENT           5.42\n6383     (20, 30]                   OWN           5.42\n21672    (30, 40]                  RENT           5.42\n###PART C \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\ncleanedData = df_train.dropna()\nfeatureCombos = [\n    ['loan_int_rate', 'loan_percent_income'], \n    ['person_age', 'person_income', 'person_emp_length', 'loan_amnt'],\n    ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income']\n]\n\nfor features in featureCombos:\n    X = cleanedData[features]\n    y = cleanedData['loan_status'] \n\n    model = LogisticRegression(max_iter=1000)\n    model.fit(X, y)\n\n    y_pred = model.predict(X)\n    #print(classification_report(y_test, y_pred))\n    coef = model.coef_\n    print(coef)\n\n    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n    print(\"Cross-validation scores:\", scores)\n\n[[0.28951836 8.41878466]]\nCross-validation scores: [0.82572614 0.82354226 0.82922035 0.81476627 0.82350371]\n[[ 2.26769069e-03 -3.25488772e-05 -3.34182406e-02  1.15567329e-04]]\nCross-validation scores: [0.79537017 0.80061149 0.80082988 0.79663609 0.79750983]\n[[-8.69205819e-04 -1.19666596e-06 -2.07101202e-02 -7.94597482e-05\n   3.13264706e-01  1.09583696e+01]]\nCross-validation scores: [0.83751911 0.83380651 0.83358812 0.82459589 0.83355177]\nimport numpy as np \n\nprobs = model.predict_proba(X)[:, 1]\nthresholds = np.linspace(0, 1, 100)\npotentialAvgProfits = []\n\nfor threshold in thresholds:\n    predictions = probs &gt;= threshold\n    profitPer = np.where(predictions == 1,\n                                   X['loan_amnt'] * (1 + 0.25 * X['loan_int_rate']) ** 10 - X['loan_amnt'],\n                                   X['loan_amnt'] * (1 + 0.25 * X['loan_int_rate']) ** 3 - 1.7 * X['loan_amnt'])\n    \n    avgProfit = profitPer.mean()\n    potentialAvgProfits.append(avgProfit)\n\n\nthreshIndex = np.argmax(potentialAvgProfits)\nbestThresh = thresholds[threshIndex]\nbestAvgProfit = potentialAvgProfits[threshIndex]\nprint(bestThresh)\nprint(bestAvgProfit)\n\n0.0\n31508209740.26809\n#load test set \n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ncleanedTestData = df_test.dropna()\nX_test = cleanedTestData[features]\n\nprobs = model.predict_proba(X_test)[:, 1]\n\nfor threshold in thresholds:\n    predictions = probs &gt;= threshold\n    profitPer = np.where(predictions == 1,\n                                   X_test['loan_amnt'] * (1 + 0.25 * X_test['loan_int_rate']) ** 10 - X_test['loan_amnt'],\n                                   X_test['loan_amnt'] * (1 + 0.25 * X_test['loan_int_rate']) ** 3 - 1.7 * X_test['loan_amnt'])\n    \n    avgProfit = profitPer.mean()\nprint(avgProfit)\n\n611638.5685903775\n#Part F \n\n#SHOULD FIRST ADD IN THE RESULTS OF MY MODEL INTO A NEW TEST DATA SET, evaluateModelData\n\nageCategory = pd.cut(df_test['person_age'], bins=[20, 30, 40, 50, 60, 70, 80, 90, 100])\nageCatCounts = df_test.groupby(ageCategory)['loan_status'].count()\nageCatApproved = df_test.groupby(ageCategory)['loan_status'].mean()\n\n\napprovalsByLI = df_test.groupby('loan_intent')['NEWCOLUMN'].mean() \ndefaultRates = df_test.groupby('loan_intent')['NEWCOLUMN'].mean()\n\nincomeCategory = pd.cut(df_test['person_income'], bins=[0, 50000, 100000, 150000, 200000, np.inf])\napprovalsbyInc = df_test.groupby(incomeCategory)['NEWCOLUMN'].mean()  # proportion of approvals by income level"
  },
  {
    "objectID": "posts/Decision Making/index.html#part-g",
    "href": "posts/Decision Making/index.html#part-g",
    "title": "Optimal Decision Making",
    "section": "Part G",
    "text": "Part G\nConsidering that people seeking loans for medical expense have high rates of default, is it fair that it is more difficult for them to obtain access to credit?\nYou are free to define “fairness” in a way that makes sense to you, but please write down your definition as part of your discussion."
  },
  {
    "objectID": "posts/Decision Making/index.html#conclusion",
    "href": "posts/Decision Making/index.html#conclusion",
    "title": "Optimal Decision Making",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Palmer Penguins/index.html",
    "href": "posts/Palmer Penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The aim of this exploration of the Palmer Penguins dataset is to gain insight into the dataset through predictive classification. This exploration involves constructing visualizations of the data using seaborn and creating informative summary tables using pandas. By testing various combinations of models through a reproducible process, three key features are able to be identified. Out of the three features, I found that the combination of 1 qualitative feature: Clutch Completion, and 2 quantitative features: Culmen Length and Culmen Depth, were the most effective to reach 100% testing classification accuracy using a Decision Tree Model. The evaluation phase provides a visual representation of the models behavior by showing the decision regions of the trained decision tree model. Through this process, we gain a comprehensive understanding of the Palmer Penguins dataset and demonstrate the effectiveness of a trained decision tree model in accurately classifying Palmer Penguin data.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nWe can take a peek at the data to get a better idea of where to start, by using .head()\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/Palmer Penguins/index.html#figure-findings",
    "href": "posts/Palmer Penguins/index.html#figure-findings",
    "title": "Palmer Penguins",
    "section": "Figure Findings",
    "text": "Figure Findings\nVarious summary tables and visualizations were tested, aiming to determine the most effective presentation of the Palmer Penguins dataset. Initially different grouping criteria and variables were considered, such as species, island, sex, clutch completion, culmen depth and length, body mass, and flipper length. Ultimately I decided to come back and choose the summary table and visualizations that reflected the variables I later found to have the highest predictive accuracy: Culmen Depth, Culmen Length, and Clutch Completion. Summary tables were created using pandas.groupby().aggregate. From the summary tables we observe that among different species, Culmen Length varies to a larger degree than Culmen Depth does. Upon a glance of the summary table, it is hard to see if clutch completion and culmen characteristics are correlated. In the first visualization on the right, we can see the penguins depicted by virtue of the respective culmen length and depth, colored by the Clutch Completion. We observe a relatively uniform distribution of clutch completion- there does not seem to be an immediate observable relationship between the three. My next visualization shows the same axes of Culmen Depth and Length, this time however colored by species. In this figure, we notice clear species groupings, corresponding to their Culmen Depth-Length relationship."
  },
  {
    "objectID": "posts/Palmer Penguins/index.html#find-the-best-features-for-prediction",
    "href": "posts/Palmer Penguins/index.html#find-the-best-features-for-prediction",
    "title": "Palmer Penguins",
    "section": "Find the Best Features for Prediction",
    "text": "Find the Best Features for Prediction\nIn order to ensure our model has the highest classification accuracy possible, we need to select features that are more indicative of the species group a penguin belongs to. By testing every combination of features, we are able to find the three that result in the highest training set accuracy. We find that there are multiple 3-feature sets with 100% training set classification accuracy.\n\nfrom itertools import combinations\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Flipper Length (mm)','Culmen Length (mm)', 'Culmen Depth (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Culmen Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Culmen Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Culmen Length (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Culmen Depth (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nWe can see that the combinations of variables:\n[‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Flipper Length (mm)’, ‘Culmen Length (mm)’] [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Flipper Length (mm)’, ‘Culmen Depth (mm)’] [‘Clutch Completion_No’, ‘Clutch Completion_Yes’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Flipper Length (mm)’, ‘Culmen Length (mm)’] [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Flipper Length (mm)’, ‘Culmen Depth (mm)’] [‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’] [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Flipper Length (mm)’, ‘Culmen Length (mm)’] [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Flipper Length (mm)’, ‘Culmen Depth (mm)’] [‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’, ‘Culmen Length (mm)’, ‘Culmen Depth (mm)’]\nAll result in 100% testing accuracy.\nIn order to narrow these feature-sets down to the single set that will be the most effective and give us not only 100% training set classificaiton accuracy but also 100% testing set classification accuracy, we repeat the process from earlier. Now operating on the testing set, we test each combination of variables that resulted in 100% training accuracy, to see if any combination of the variables also result in 100% testing accuracy. This is done by scoring models built respectively on each combination of variables. We narrow down the 3-feature sets into one final best feature set: Clutch Completion, Culmen Length (mm), and Culmen Depth (mm).\n\n#pretend we're numbering these from 1-9, look at testing accuracy for each one that has 1.0. \n#3 1.0 !!!  ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\ntempCols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']\nDT = DecisionTreeClassifier()\nDT.fit(X_train[tempCols], y_train)\nprint(DT.score(X_train[tempCols], y_train))\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nDT.score(X_test[tempCols], y_test)\n\n1.0\n\n\n1.0\n\n\nWe check our cross-validation score, noting that a cross validation score of 94.5% is indicitive of future model generalizability.\n\n#cross-validation\nfrom sklearn.model_selection import cross_val_score\ncv_scores_DT = cross_val_score(DT, X_train, y_train, cv=5)\ncv_scores_DT.mean()\n\n0.9569381598793363\n\n\n\nPlotting Decision Regions\nFinally, we can visualize these Decision Regions using matplotlib.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n#visualizing decision regions on train set \nplot_regions(DT, X_train[tempCols], y_train)\n\n\n\n\n\n\n\n\n\n#visualizing decision regions on test set\nplot_regions(DT, X_test[tempCols], y_test)\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\n\n#need to truncate X_test to have only the columns we're using to predict before doing confusion matrix\nX_test = X_test[['Culmen Length (mm)', 'Culmen Depth (mm)', 'Clutch Completion_No', 'Clutch Completion_Yes']]\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = DT.predict(X_test)\ny_test_pred\n\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nSince the trained model has 100% training and testing classifcation accuracy, the confusion matrix reflects the lack of classification errors.\n\n\nSummary Discussion\nIn conclusion, this exploration of the Palmer Penguins dataset has provided insights into the dataset through predictive classification. Through the construction of informative summary tables and visually appealing figures, we were able to uncover various patterns, trends, and relationships within the data. By systematically testing different combinations of both qualitative and quantitative features of the Palmer Penguins, I found the best combination of three features that yielded high classification accuracy. One qualitative feature, clutch completion, and two quantitative features, culmen length and culmen depth (mm). By employing scikit-learn’s DecisionTreeClassifier, I trained a model on these features and achieved a classification accuracy of 100% on both the training and testing sets. This shows the importance of feature selection, model selection and evaluation in building accurate predictive models. Through this whole process, I gained a comprehensive understanding of the Palmer Penguins dataset and demonstrate the effectiveness of a trained model in accurately classifying Palmer Penguin data. From this process, I learned how important both the selection of the right features as well as the right model are. I learned that 100% training set classification accuracy does not guarantee 100% testing set classification accuracy, and how it is important to tweak the model to ensure that future uses beyond a training set maintain a high level of accuracy. I gained an appreciation of visualizations as a tool for greater understanding of initial data as well as the effect of models. I feel confident in future endeavors to explore, visualize, and predict data."
  },
  {
    "objectID": "posts/Racial Bias/index.html",
    "href": "posts/Racial Bias/index.html",
    "title": "Dissecting Racial Bias",
    "section": "",
    "text": "Abstract\nThis exploratory analysis aims to replicate the primary findings of Obermeyer et al. in their study of ‘Dissecting racial bias in an algorithm used to manage the health of populations’ (2019). By reproducing various figures, such as Mean Number of Chronic Illnesses by Percentile Risk Score and Total Medical Expenditure vs. Number of Chronic Illnesses, as well as fitting a linear regression model, this analysis aims to determine if there exists substansial disparity between the risk score and medical costs incurred of black and white patients. The findings indicate that there does indeed exists a disparity between both the attributed risk score and the medical costs incurred of the patients by race. The risk score of white patients are higher than their black patient counterparts who have the same number of chronic illnesses. The costs follow a similar pattern, where white patients incur greater medical costs than their black patient counterparts who have the same number of chronic illnesses. Black patients incur approximately 75% of the costs that white patients incur. These finding suggest that black patients are not getting the health care they need, as they are deprioritized with lower risk scores and receiving less needed healthcare as they take on less medical costs.\n\n\nPART A: Data Access\nLoading in the data, I take a preliminary view to understand the meaning of the respective rows and columns.\n\n# load data \nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\nPART B: Reproducing Figure 1\n\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n\nblackDF= df[df[\"race\"] == \"black\"].copy()\nblackDF['risk_percentile'] = (df['risk_score_t'].rank(pct=True) * 100).round()\nmeanCCblack = blackDF.groupby('risk_percentile')['gagne_sum_t'].transform('mean')\nblackDF['meanCCblack'] = meanCCblack\n\n \nwhiteDF = df[df[\"race\"] == \"white\"].copy()\nwhiteDF['risk_percentile'] = (df['risk_score_t'].rank(pct=True) * 100).round()\nmeanCCwhite = whiteDF.groupby('risk_percentile')['gagne_sum_t'].transform('mean')\nwhiteDF['meanCCwhite'] = meanCCwhite\n\n\n\nsns.scatterplot(x='meanCCblack', y='risk_percentile', data=blackDF, color='teal', label='Black')\nsns.scatterplot(x='meanCCwhite', y='risk_percentile', data=whiteDF, color='orange', label='White')\n\nplt.xlabel('Mean Number of Chronic Illnesses by Percentile Risk Score')\nplt.ylabel('Percentile Risk Score')\nplt.title('Mean Number of Chronic Illnesses')\nplt.legend(title='Race')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSupposing that Patient A is black and Patient B is white, we can see from the plot, Patient A and Patient B are not equally likely to be referred to the high-risk care management program. Given the same number of active chronic conditions, white patients tend to score higher risk scores compared to their black counterparts. Thus white patients, such as patient A would be more likely to be referred to the high-risk care management program than black patients such as patient B. As Obermeyer et al said, “less-healthy Blacks scored at similar risk scores to more-healthy Whites” and thus there is evidence for “substantial disparities” in the program.\n\n\nPART C: Reproducing Figure 3\n\ndf['risk_percentile'] = (df['risk_score_t'].rank(pct=True) * 100).round()\nriskCostDF = df.groupby(['risk_percentile','race']).aggregate({'cost_t': 'mean'}).reset_index()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nsns.scatterplot(x='risk_percentile', y='cost_t', hue = 'race', data=riskCostDF, ax=axes[0])\naxes[0].set_xlabel('Percentile Risk Score')\naxes[0].set_ylabel('Total Medical Expenditure')\naxes[0].set_title('Total Medical Expenditure vs. Percentile Risk Score')\naxes[0].grid(True)\naxes[0].set_yscale('log')\n\nblackDF = df[df[\"race\"] == \"black\"].copy()\nwhiteDF = df[df[\"race\"] == \"white\"].copy()\n\n#second graph \nchronicCostBlack = blackDF.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicCostWhite = whiteDF.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\n\nsns.scatterplot(x='gagne_sum_t', y='cost_t', data=chronicCostBlack, label =\"White\", ax=axes[1])\nsns.scatterplot(x='gagne_sum_t', y='cost_t', data=chronicCostWhite, label =\"Black\", ax=axes[1])\naxes[1].set_xlabel('Number of Chronic Illnesses')\naxes[1].set_ylabel('Total Medical Expenditure')\naxes[1].set_title('Total Medical Expenditure vs. Number of Chronic Illnesses')\naxes[1].legend(title='Race')\naxes[1].grid(True)\n\nplt.tight_layout(pad=3.0) \nplt.show()\n\n\n\n\n\n\n\n\nFrom the reproduction of these two figures, we can better understand our data. We note that a vast majority of patients in this data set have 5 or fewer chronic conditions. A surprising finding that there exists more disparity in health conditional risk than in costs. However, black patients have lower costs on average than their white patient counterparts, potentially suggesting they are not getting the care they need. To accurately predict costs for patients, their race is crucial information as it distinguishes their predicted costs, this suggests that the relationship between the number of chronic conditions and the cost might be nonlinear.\n\n\nPART D: Modeling Cost Disparity\n\nCostDisparityDF = df[df['gagne_sum_t'] &lt;= 5]\nfiveChronics = CostDisparityDF.shape[0]\ntotalPatientsNum = df.shape[0]\npercentFiveChronics = (fiveChronics / totalPatientsNum) * 100\nprint(percentFiveChronics)\n\n95.53952115447689\n\n\nAbout 95 percent of patients have five or less chronic conditions. Since this is the vast majority of patients, it is justified to focus in on these patients\n\nCostDisparityDF = CostDisparityDF[CostDisparityDF['cost_t'] &gt; 0]\nCostDisparityDF['logCost'] = np.log(CostDisparityDF['cost_t'])\n\nCostDisparityDF['OneHotRace'] = (CostDisparityDF['race'] == 'black').astype(int)\n\n#separate into predictor variables and target variable \npredictorX = CostDisparityDF[['OneHotRace', 'gagne_sum_t']]\ntargetY = CostDisparityDF['logCost']\n\n\n\ntargetY\n\n0        7.090077\n1        7.863267\n2        6.214608\n3        7.170120\n4        7.003065\n           ...   \n48779    6.684612\n48780    7.696213\n48781    6.684612\n48782    7.170120\n48783    8.389360\nName: logCost, Length: 44748, dtype: float64\n\n\nAs indicated in the findings from my figures above, the relationship between the number of chronic conditions and the cost might be nonlinear. Thus in order to fit a linear regression model, we must fit the model with a certain number of polynomial features of active chronic condition in order to to account for the nonlinearity. To determine how many polynomial features should be used for best predictions on this data set, cross validation is needed to test various data sets with differing number of polynomial feature sizes. For each possible polynomial feature size, a LinearRegression model is constructed a cross validation score is computed.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\n\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.model_selection import cross_val_score\n#test each num of active chronic conditions \ncvScores = []\n\nfor degree in range (1,6):\n    X_poly = add_polynomial_features(predictorX, degree)\n    modelToEvaluate = LR()\n    scores = cross_val_score(modelToEvaluate, X_poly, targetY, cv=5, scoring='explained_variance')\n    cvScores.append(np.mean(scores))\n\n    modelToEvaluate.fit(X_poly, targetY)\n\n\ncvScores\n\n[0.08547350567418092,\n 0.08547361170477137,\n 0.08629426236939855,\n 0.08705037980708544,\n 0.08740844467965055]\n\n\n\n#fit the one that is best \npredictorX = add_polynomial_features(predictorX,6)\nfinalModel = LR()\nfinalModel.fit(predictorX, targetY)\nfinalModel.coef_\n\narray([-0.2827181 ,  0.5939195 ,  0.5939195 , -1.28588299,  0.65937337,\n       -0.14306628,  0.01107307])\n\n\nOnce the size of the polynomial feature that is most resonable for this dataset is determined, I fit one last linear regression model with the correct number of polynomial features, which in this case, is all 5.\n\ncostIncurred = np.exp(finalModel.coef_[0])\ncostIncurred\n#0.75\n\n0.7537322331638742\n\n\nFinally, e^W(b) is computed, which represents the cost incurred by black patients as a percentage of white patients. We find this number to be 0.75, which means that black patients incur 75% of the cost that white patients incur. This finding does indeed roughly support the argument of Obermeyer et al. (2019), as Obermeyer et al. claimed that there were opportunites for a disparity wedge to occur between needing health care and receiving health care, claiming that this disparity wedge was found to be correlated with race. Our findings indicate the same thing, since the interpretation of our 75% cost incurred figure could mean that a black patient with the same amount of chronic illnesses as a white patient would be paying less (and thus assumed to be receiving less needed healthcare).\n\n\nPart E: Discussion\nThroughout this exploratory analysis that aimed to replicate the primary findings of Obermeyer et al. in their study of ‘Dissecting racial bias in an algorithm used to manage the health of populations’ (2019), we find similar results and conclusions. By contructing plot figures and linear regression models, I learned that there does indeed exists a disparity between both the attributed risk score and the medical costs incurred of the patients by race. The risk score of white patients are higher than their black patient counterparts who have the same number of chronic illnesses. The costs follow a similar pattern, Black patients incurring approximately 75% of the costs that white patients incur, where both sets of patient counterparts have the same number of chronic illnesses. These finding suggest that black patients are not getting the health care they need, as they are deprioritized with lower risk scores and receiving less needed healthcare as they take on less medical costs. Thus this model violates the fairness measure of error rate parity. The importance of constructing a linear regression model with the proper number of polynomial features was emphasized in this analysis. Additionally, I gained an appreciation for extrapolating findings beyond their face value. Black patients incur less medical costs than their white counterparts with the same number of chronic illnesses. Why? Perhaps because they are not receiving the full extent of the healthcare they need and are thus paying less medical costs.\nWhat aspects of the study support your answer?\nOf the three statistical discrimation criteria discussed in Chapter 3 of Barocas, Hardt, and Narayanan (2023)- error rate parity, acceptance rate parity/independence, and sufficiency (calibration), the criteria that best describes the purported bias of the algorithm studied by Obermeyer et al. (2019) is error rate parity. Barocas, Hardt, and Narayanan (2023) define error rate parity as one of the measures of statistical fairness in which the error rates should be balanced across different groups, such as in this study the error rates should be balanced across different demographic groups. Acceptance rate parity/independence refers to the principle that people with similar qualifications should have equal probabilities of being accepted into a program or receiving a service, while sufficiency (calibration) is the idea that that predicted probabilities of outcomes actually reflect he true likelihood of the events occurring. I believe that error rate parity best describes the purported bias of the algorithm because the algorithm systematically assigns lower risk scores to black patients compared to white patients with the same number of chronic illnesses which can/does lead to unequal treatment and resource allocation in healthcare. This is reflected by the 75% medical costs that Black patients pay compared to their White counterparts. Black patients are getting assigned lower scores and thus pay less for a lesser degress of medical treatment. The aspects of the study that support my answer is Obermeyer et al.’s commentary that this bias is “attributable to label choice—the difference between some unobserved optimal prediction and the prediction of an algorithm trained on an observed label”. They claim that labels can be often measured in an error-prone way that reflect structural inequalities. This model has a problem with error rate parity as it does not make errors at similar rates for different groups to ensure fairness."
  },
  {
    "objectID": "posts/MidCourseEvals/index.html",
    "href": "posts/MidCourseEvals/index.html",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "Julia Joy\n\n\n\n\n\n*How often have you attended class? I have attended every class except for 1.\nHow often have you taken notes on the core readings ahead of the class period? I always read the readings, but I probably take notes on them maybe 1/2-1/3 of the time?\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? Almost always, probably with the exception of 1 or 2 times.\nHow many times have you actually presented the daily warm-up to your team? I cant quite remember, maybe 3 or 4?\nHow many times have you asked your team for help while presenting the daily warm-up? I think for 1 part of a problem once.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? I feel like I can frequently learn a little something new, even if I did the warmup correctly it is always informational to see other approaches or when we were having more discussion-based warmups I learned other perspectives to see things.\nHow often have you helped a teammate during the daily warm-up presentation? Once or twice but we always contribute to each other.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? I have not attended Student Hours.\nHow often have you asked for or received help from your fellow students? I have a lot of friends in the class so we often meet together in a study group, so I probably ask someone a question around once a week whether its about the warmups, blog posts, or general clarification of things were supposed to be doing.\nHave you been regularly participating in a study group outside class? Yes.\nHow often have you posted questions or answers in Slack? I have maybe posted one question.\n\n\n\n\n\nHow many blog posts have you submitted? I have submitted 3 blog posts and am currently working on 2 more.\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: 1\nM: Revisions useful: 2\nR: Revisions encouraged: 0\nN: Incomplete: 0\n\nRoughly how many hours per week have you spent on this course outside of class? Roughly 9-11 depending on the week.\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nIn the beginning, I expressed an interest in the Implementation category, as well as a tentative on in the theory category. I think I have definitely focused in on the Implementation category. I always feel like more practice with effective coding and learning how to use new tools is so invaluable, I feel like I have been able to continue that in this course with the learning of all of our new models like the Perceptron. In general too expanding my coding skills by getting more familiar with python and different packages like torch. I am excited to continue with more different kinds of Machine Learning algorithms and models! However for the theory category, I do not think I really would like to focus in on the category as much as I originally thought. I forgot how long it had been since I took linear algebra my freshman year, and I have never taken multi-variable calculus. I can obviously handle differentiation, and my mathematical knowledge/general understanding is definitely not lacking, but I realized that I have less interest in intense mathematical descriptions of frameworks and algorithms, I enjoy my little problem sets more. Thus I do not think I will be choosing to focus in on the Theory category that much anymore. I will focus more into the Implementation category as I have been. Our third(?) week in the semester too maybe me more interested in the social responsibility side of machine learning algorithms, so potentially it would be interesting to conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm for my final project? We shall see!\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nIn my original goal setting, I aimed to have a “Satisfactory/No Revision Required” rating on 5-6 of my of my blog posts, and perhaps completing more (I want to complete at least around 60-80% of blog posts) but leaving some space for some blog posts to remain in the “needs revision category”.”\nI hadn’t looked closely enough at the way blog posts were graded yet I guess, because now I know there are two categories that could satisfy that first part of my blog post goals: E: No revisions suggested: and M: Revisions useful:*. Ideally all would be E’s, but I think I will leave the space for 1-2 blog posts to remain in the M category, as I have recieved enough feedback at this point to understand the difference between an E and M blog post, and that stays in accordance with my goals. Additionally in the second part of my goals for blog posts I said I want to complete around 60-80% of blog posts. After getting through half of the semester, I can understand now that completing 80% of the blog posts is simply out of reach for me in this class, especially as we approach our final projects starting. I also did not realize until after my goal setting that some weeks there would be 2 blog posts that could be completed, so that puts it more in perspective as well as I would only choose 1 of 2 options to complete and not both. I think my goal of completing roughly 60% of blog posts is good, as it is also in accordance with the number of 5-6 solid good work blog posts.\nCurrently I have completed 3 blog posts (E,M,M), and have 2 more underway. There have been a total of 7 blog posts out that we could complete, so I am also doing well in my aim of a 60% completion ratio.\nI think that in my goals for the blog posts, I am generally on track to meet my goals from the beginning of the course! I will say that in my goal setting I did mention that ideally it would take me 1-1.5 weeks to complete a blog post and I have struggled to keep up with that timeline, due to other classwork and the fact that I find myself spending considerable amounts of time to thoroughly understand the warmups. I am trying to get better at this timeline, and I think after this week and my 2 in progress blog posts, I will be able to stick to the timeline better. I do think I place more importance in the actual progress of the blog posts rather than the speed at which I turn them in, so I think all should be well for my blog post goals!\n\n\n\nOverall, I definitely think I am on track and acting in accordance with my course presence/participation goals I set.\nI have missed 1 class due to sickness, so I am still very much in line with my goal to attend nearly every class unless truly having to miss it (like when I was sick) or with the personal exception of 2/3 absences. In my goal setting I had aimed to complete every warmup before class and I can happily and proudly say that I have done so for every class period in the semester so far! So my leeway of one or two missed warmups is still very much intact as well. I have also stuck in line of my goals of doing most of the course readings as well as thoroughly participating in our small warmup groups. I have yet to attend peer hours, but I have attended office hours a few times when the need struck which is what I expected and set as an outline for myself. I was right in my guessing that I would not be the biggest slack user, but I have asked all the questions I needed answered in some form or another which is good. Study groups have been big for me this semester too, since I know a good amount of people in the course and find it helpful to talk out some problems I run into, or to confirm things we discuss in class.\nI do think I struggle to raise my hand to answer questions in class, my shyness outweighing everything else. It is something I have tried to work on for a long time, so I have a continued awareness for it in this class as well.\n\n\n\nWe still have not started the final project yet, although the project proposal brainstorming is due on Tuesday (as I write this on Friday), so I still do not have super concrete goals for my project. I have the same goals as before, “I would like to complete all project milestones roughly on time to keep myself and the group accountable in our semester time frame, contribute to the project in an equal manner including in the algorithm implementation and project presentation, and do what I can to be a great group project partner.”, and I do think I will be able to stick in accordance with those goals as our project approaches. This weekend I need to brainstorm what I would like to do for our final project. I think that studying a complex data set in depth, or conducting a series of experiments related to assessing algorithmic bias in a certain class of algorithm interests me more than implementing a new algorithm. However I will also draw inspiration from other peoples project pitches and who knows maybe I will discover something new and interesting to focus in on for our final project.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nNot that I can think of right now!\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nI think I have lightly done this throughout this course evaluation and reflection process, but here are the alterations stated plainly.\n\nI had stated a tentative focus on the theory category alongside my primary focus of the implementation category. I no longer wish to focus in as much on the theory category, and instead will apply my efforts to my original focus of the implementation category. Maybe even exploring the social responsibility side of machine learning algorithms in the form of a final project? I did find those weeks of discussion quite interesting, but this is more up to how the final project brainstorming go and if I find my interest sparked by anything.\nIn my original goal of completing 60%-80% of blog posts, after half a semester and a more thorough understanding of how many blog posts we are assigned to be able to do, completing 80% of blog posts is not achievable for me. So I will stick with the lower limit of 60%, or aka 5-6 blog posts.\n\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A\n\n\nA way in which I resonate with the soundbytes for that grade above is… - I am proud of my time in this course. - I am ready to take the techniques and ideas of this course into my future. - I have acted in accordance with my goals I set for myself at the beginning of the semester.\n\nUpon completing this mid-course reflection and evaluation, I am very proud that I have stuck in accordance with my goals. I feel that I have acted as a hard-working student in this course, barely deviating from any goals and putting in the time and work that I wanted to see myself do in this class. For these reasons I feel like my learning participation and achievement in CSCI 0451 so far are best reflected by a grade of A. I hope to stick in accordance with my goals for the remainder of the semester and as our final project approaches.\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\n[your response here]"
  },
  {
    "objectID": "posts/MidCourseEvals/index.html#the-data",
    "href": "posts/MidCourseEvals/index.html#the-data",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "*How often have you attended class? I have attended every class except for 1.\nHow often have you taken notes on the core readings ahead of the class period? I always read the readings, but I probably take notes on them maybe 1/2-1/3 of the time?\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? Almost always, probably with the exception of 1 or 2 times.\nHow many times have you actually presented the daily warm-up to your team? I cant quite remember, maybe 3 or 4?\nHow many times have you asked your team for help while presenting the daily warm-up? I think for 1 part of a problem once.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? I feel like I can frequently learn a little something new, even if I did the warmup correctly it is always informational to see other approaches or when we were having more discussion-based warmups I learned other perspectives to see things.\nHow often have you helped a teammate during the daily warm-up presentation? Once or twice but we always contribute to each other.\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? I have not attended Student Hours.\nHow often have you asked for or received help from your fellow students? I have a lot of friends in the class so we often meet together in a study group, so I probably ask someone a question around once a week whether its about the warmups, blog posts, or general clarification of things were supposed to be doing.\nHave you been regularly participating in a study group outside class? Yes.\nHow often have you posted questions or answers in Slack? I have maybe posted one question.\n\n\n\n\n\nHow many blog posts have you submitted? I have submitted 3 blog posts and am currently working on 2 more.\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: 1\nM: Revisions useful: 2\nR: Revisions encouraged: 0\nN: Incomplete: 0\n\nRoughly how many hours per week have you spent on this course outside of class? Roughly 9-11 depending on the week."
  },
  {
    "objectID": "posts/MidCourseEvals/index.html#what-youve-learned",
    "href": "posts/MidCourseEvals/index.html#what-youve-learned",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "At the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nIn the beginning, I expressed an interest in the Implementation category, as well as a tentative on in the theory category. I think I have definitely focused in on the Implementation category. I always feel like more practice with effective coding and learning how to use new tools is so invaluable, I feel like I have been able to continue that in this course with the learning of all of our new models like the Perceptron. In general too expanding my coding skills by getting more familiar with python and different packages like torch. I am excited to continue with more different kinds of Machine Learning algorithms and models! However for the theory category, I do not think I really would like to focus in on the category as much as I originally thought. I forgot how long it had been since I took linear algebra my freshman year, and I have never taken multi-variable calculus. I can obviously handle differentiation, and my mathematical knowledge/general understanding is definitely not lacking, but I realized that I have less interest in intense mathematical descriptions of frameworks and algorithms, I enjoy my little problem sets more. Thus I do not think I will be choosing to focus in on the Theory category that much anymore. I will focus more into the Implementation category as I have been. Our third(?) week in the semester too maybe me more interested in the social responsibility side of machine learning algorithms, so potentially it would be interesting to conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm for my final project? We shall see!"
  },
  {
    "objectID": "posts/MidCourseEvals/index.html#reflecting-on-goals",
    "href": "posts/MidCourseEvals/index.html#reflecting-on-goals",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "For each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nIn my original goal setting, I aimed to have a “Satisfactory/No Revision Required” rating on 5-6 of my of my blog posts, and perhaps completing more (I want to complete at least around 60-80% of blog posts) but leaving some space for some blog posts to remain in the “needs revision category”.”\nI hadn’t looked closely enough at the way blog posts were graded yet I guess, because now I know there are two categories that could satisfy that first part of my blog post goals: E: No revisions suggested: and M: Revisions useful:*. Ideally all would be E’s, but I think I will leave the space for 1-2 blog posts to remain in the M category, as I have recieved enough feedback at this point to understand the difference between an E and M blog post, and that stays in accordance with my goals. Additionally in the second part of my goals for blog posts I said I want to complete around 60-80% of blog posts. After getting through half of the semester, I can understand now that completing 80% of the blog posts is simply out of reach for me in this class, especially as we approach our final projects starting. I also did not realize until after my goal setting that some weeks there would be 2 blog posts that could be completed, so that puts it more in perspective as well as I would only choose 1 of 2 options to complete and not both. I think my goal of completing roughly 60% of blog posts is good, as it is also in accordance with the number of 5-6 solid good work blog posts.\nCurrently I have completed 3 blog posts (E,M,M), and have 2 more underway. There have been a total of 7 blog posts out that we could complete, so I am also doing well in my aim of a 60% completion ratio.\nI think that in my goals for the blog posts, I am generally on track to meet my goals from the beginning of the course! I will say that in my goal setting I did mention that ideally it would take me 1-1.5 weeks to complete a blog post and I have struggled to keep up with that timeline, due to other classwork and the fact that I find myself spending considerable amounts of time to thoroughly understand the warmups. I am trying to get better at this timeline, and I think after this week and my 2 in progress blog posts, I will be able to stick to the timeline better. I do think I place more importance in the actual progress of the blog posts rather than the speed at which I turn them in, so I think all should be well for my blog post goals!\n\n\n\nOverall, I definitely think I am on track and acting in accordance with my course presence/participation goals I set.\nI have missed 1 class due to sickness, so I am still very much in line with my goal to attend nearly every class unless truly having to miss it (like when I was sick) or with the personal exception of 2/3 absences. In my goal setting I had aimed to complete every warmup before class and I can happily and proudly say that I have done so for every class period in the semester so far! So my leeway of one or two missed warmups is still very much intact as well. I have also stuck in line of my goals of doing most of the course readings as well as thoroughly participating in our small warmup groups. I have yet to attend peer hours, but I have attended office hours a few times when the need struck which is what I expected and set as an outline for myself. I was right in my guessing that I would not be the biggest slack user, but I have asked all the questions I needed answered in some form or another which is good. Study groups have been big for me this semester too, since I know a good amount of people in the course and find it helpful to talk out some problems I run into, or to confirm things we discuss in class.\nI do think I struggle to raise my hand to answer questions in class, my shyness outweighing everything else. It is something I have tried to work on for a long time, so I have a continued awareness for it in this class as well.\n\n\n\nWe still have not started the final project yet, although the project proposal brainstorming is due on Tuesday (as I write this on Friday), so I still do not have super concrete goals for my project. I have the same goals as before, “I would like to complete all project milestones roughly on time to keep myself and the group accountable in our semester time frame, contribute to the project in an equal manner including in the algorithm implementation and project presentation, and do what I can to be a great group project partner.”, and I do think I will be able to stick in accordance with those goals as our project approaches. This weekend I need to brainstorm what I would like to do for our final project. I think that studying a complex data set in depth, or conducting a series of experiments related to assessing algorithmic bias in a certain class of algorithm interests me more than implementing a new algorithm. However I will also draw inspiration from other peoples project pitches and who knows maybe I will discover something new and interesting to focus in on for our final project.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nNot that I can think of right now!\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nI think I have lightly done this throughout this course evaluation and reflection process, but here are the alterations stated plainly.\n\nI had stated a tentative focus on the theory category alongside my primary focus of the implementation category. I no longer wish to focus in as much on the theory category, and instead will apply my efforts to my original focus of the implementation category. Maybe even exploring the social responsibility side of machine learning algorithms in the form of a final project? I did find those weeks of discussion quite interesting, but this is more up to how the final project brainstorming go and if I find my interest sparked by anything.\nIn my original goal of completing 60%-80% of blog posts, after half a semester and a more thorough understanding of how many blog posts we are assigned to be able to do, completing 80% of blog posts is not achievable for me. So I will stick with the lower limit of 60%, or aka 5-6 blog posts."
  },
  {
    "objectID": "posts/MidCourseEvals/index.html#grade-and-goals",
    "href": "posts/MidCourseEvals/index.html#grade-and-goals",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "Take 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A\n\n\nA way in which I resonate with the soundbytes for that grade above is… - I am proud of my time in this course. - I am ready to take the techniques and ideas of this course into my future. - I have acted in accordance with my goals I set for myself at the beginning of the semester.\n\nUpon completing this mid-course reflection and evaluation, I am very proud that I have stuck in accordance with my goals. I feel that I have acted as a hard-working student in this course, barely deviating from any goals and putting in the time and work that I wanted to see myself do in this class. For these reasons I feel like my learning participation and achievement in CSCI 0451 so far are best reflected by a grade of A. I hope to stick in accordance with my goals for the remainder of the semester and as our final project approaches."
  },
  {
    "objectID": "posts/MidCourseEvals/index.html#optional-how-to-improve",
    "href": "posts/MidCourseEvals/index.html#optional-how-to-improve",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "You may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\n[your response here]"
  },
  {
    "objectID": "posts/WIDS Conference/index.html",
    "href": "posts/WIDS Conference/index.html",
    "title": "WiDS Conference at Middlebury College",
    "section": "",
    "text": "In this blog post, I reflect on my experiences reading ‘Solving the Equation: The Variables for Women’s Success in Engineering and Computing’ by Corbett and Hill (2015) and attending the Women in Data Science conference. Through the two of these experiences, I gained valuable insights into the challenges and potential solutions surrounding gender diversity in STEM fields. I learned about the systemic barriers that hinder women’s ability to enter, continue, and succeed in engineering and computing, that highlight the importance of proactive strategies to equalize representation of women in STEM. This blog post outlines these learnings by reflecting on questions from ‘Solving the Equation: The Variables for Women’s Success in Engineering and Computing’ as well as summarizing the speakers from the Women in Data Science conference."
  },
  {
    "objectID": "posts/WIDS Conference/index.html#part-1-why-spotlight-women-in-data-science",
    "href": "posts/WIDS Conference/index.html#part-1-why-spotlight-women-in-data-science",
    "title": "WiDS Conference at Middlebury College",
    "section": "Part 1: Why Spotlight Women in Data Science?",
    "text": "Part 1: Why Spotlight Women in Data Science?\nIn ‘Solving the Equation: The Variables for Women’s Success in Engineering and Computing’ by Corbett and Hill (2015), the authors show that women are underrepresented in computing, math, and engineering. But why is it a problem? And for whom is it a problem?\nThis underrepresentation of women significant challenges on multiple fronts. This underrepresentation reflects systemic inequalities and biases that have been shown to hinder equal opportunities for women in these fields. By not teaching everyone that women can hold their own and be equally as good in STEM fields as men, women lose out on these high-quality job opportunities. This is not just a problem for women. By a lack of women in computing, math, and engineering, it not only deprives women of potential career paths, but it also limits the diversity of perspectives and talents that are crucial for solving problems, developing rounded mechanisms, and field innovation. As Corbet and Hill said, “The United States simply can’t afford to ignore the perspectives of half the population in future engineering and technical designs”. Gender biases that contribute to the underrepresentation of women in computing, math, and engineering also create obstacles for the men who work alongside women in these industries. Studies point to a devaluation of work when men work with women (West et al., 2012), which can lead to men traking steps to avoid wokring with women, worsening the challenges facing women in male-dominated fields and contributing to the problem. Everyone must work together to help balance the gender representation in computing, math, and engineering, as it is a problem that affects us all no matter our identity.\n\nWhich of the barriers and unequal challenges described in the section “Why So Few?” can be eroded by events that spotlight the achievement of women in STEM?\nThe section “Why so Few?” outlines various barriers and unequal challenges women face in the STEM field, such as sense of belonging, stereotypes and stereotype threat, challenging academic workspaces, as well as isolation. Women may face negative impacts such as experiencing difficultly balancing expected gendered home responsibilities and the culture of overwork, feel de-incentivized by the engineering culture of disengagement as studies show women are more likely than men to express a preference for work with a clear social purpose (Konrad et al., 2000), or feel isolated as significant portion of women reported feeling as if they lacked role models and mentors and did not agree with the statement that “it is safe to speak up most of the time”. While not all of these barriers or challenges can be eroded by events that spotlight the achievement of women in STEM, there are a few that can benefit from these events. What stood out most notably to me that could be benefited by these events was improving the sense of belonging for women in STEM. Events that spotlight the achievement of women in STEM can help increase the sense of belonging for women in STEM, and can have a positive domino-effect in doing so. Events that spotlight the achievement of women in STEM shows examples of women in a historically male-dominated field, helping combat both internalized and external stereotypes. These events show women that success in STEM is possible, providing role models and mentors, which was also noted as a problem for women in the industry. Achievement spotlight events are thus crucial for increasing women’s sense of belonging and increasing the number of women in STEM.\n\n\nPart 2: Attending the Women in Data Science Conference. Yay!\nBy attending the Women in Data Science Conference, I had an unique opportunity to engage with and hear from real professional women that have succeeded against all of the challenges in a male-dominated field, learning about their work as well as their experiences."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "In this blog post, I implement and experiment with gradient descent algorithms for logistic regression. I conducted three experiments, implementing Vanilla Gradient Descent, experiments to understand the benefits of momentum, and a final experiment to understand the dangers and harms of overfitting the logistic regression model on training data. I was able to make several meaningful conclusions. Implementing Vanilla Gradient Descent where beta =0 led to convergence with monotonically decreasing loss, however with benefits of momentum, such as where beta = 0.9, loss decreased at a faster rate, converging with fewer iterations than needed for Vanilla Gradient Descent. The experiment in which I fit the model too closely to the training data showed lower accuracy on the test data, warning against overfitting. I was able to learn more about logistic regression as a machine learning model throughout the implementation and exploration of this blog post."
  },
  {
    "objectID": "posts/Regression/index.html#vanilla-gradient-descent",
    "href": "posts/Regression/index.html#vanilla-gradient-descent",
    "title": "Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nThe first experiment I performed was vanilla gradient descent. When there are 2 dimensions, alpha is sufficently small, and beta is 0, I expect to see gradient descent for logistic regression converge to a weight vector w, the loss decreasing monotonically (either only increasing or only decreasing).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nlossVec = [] \n\nfor _ in range(4000):\n    # VGD alpha small and beta 0 \n    opt.step(X, y, alphaRate = 0.4, betaRate = 0)\n    loss = LR.loss(X,y)\n    lossVec.append(loss)\n\n\n#then plot loss over iterations, v similar to how we did in the perceptron blog post \ndef plotLoss(loss, label =\"\"):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(lossVec, color = \"blue\", label = label)\n    plt.scatter(torch.arange(len(loss)), loss, color = \"blue\", s=3)\n    plt.gca().set(xlabel = \"Iterations\", ylabel = \"loss\")\n    \n    plt.title(f\"Evolution of the Loss Function\")\n    #check final loss value as a means of verifying model \n    finalLossVal = loss[len(loss)-1]\n\nplotLoss(lossVec)\n\n\n\n\n\n\n\n\nFrom the plot, we can see that the loss is decreasing the entire time (and thus loss decreasing monotonically!), in line with our expected behavior of vanilla gradient descent. We can further visualize the result of the model by plotting the data with its new decision boundary.\n\ndef drawLine(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plotData (X, y, ax):\n    #from perceptron blog post, but targets now 0,1 for binary classification\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \nfig, ax = plt.subplots(1, 1)\nplotData(X, y, ax)\ndrawLine(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\n#checking the final loss to verify model\nax.set_title(f\"loss = {loss:.3f}\")\n\nText(0.5, 1.0, 'loss = 0.196')\n\n\n\n\n\n\n\n\n\nThe model did a good job of finding a separating line between the two distinct classes, with the final loss being around 0.1.\n\nBenefits of Momentum\nThe next experiment I performed was to show the benefits of momentum. On the same data, we expect to see gradient descent with momentum (when beta is a non-zero value such as 0.9) is able to converge to the correct weight vector faster, in fewer iterations than the previous experiment of vanilla gradient descent (where beta = 0).\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\nX, y = classification_data(noise = 0.2)\n\nlossVecMomentum = []\n\nfor index in range(4000):\n    # alpha small and beta nonzero = 0.9\n    opt.step(X, y, alphaRate = 0.4, betaRate = 0.9)\n    loss = LR.loss(X, y)      \n    lossVecMomentum.append(loss)\n\n\n#first plot loss function with benefits of momentum \nplotLoss(lossVecMomentum, label = \"BoM, beta = 0.9\")\n\n#add on plot of VGD\nplt.plot(lossVec, color = \"slategrey\", label = \"VDG, beta = 0\")\nplt.scatter(torch.arange(len(lossVec)), lossVec, color = \"slategrey\", s=3)\nplt.gca().set(xlabel = \"Iterations\", ylabel = \"Loss\")\nplt.legend()\n\n\n\n\n\n\n\n\nWe can see that the loss function of gradient descent with benefits of momentum (represented in blue), is decreasing faster, converging to the correct weight vector in fewer iterations than the previous experiment of vanilla gradient descent. This shows that larger non-zero beta values does allow the model to learn at a quicker rate.\n\n\nThe Dangers of Overfitting\nThe last experiment I performed is to show how overfitting can be harmful, looking at its effects. I can do so by creating na instance where I obtain 100% testing accuracy, looking at data where the number of dimensions is greater than the number of points.\nWe first implement the model on the training set, checking to see if our goal of 100% accuracy has been reached.\n\n#first the training set\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nlossVecTrain = []\nfor index in range(4000):\n    #do i need to lower alpha?\n    opt.step(X_train, y_train, alphaRate = 0.4, betaRate = 0.9)\n    loss = LR.loss(X_train, y_train) \n    lossVecTrain.append(loss)\n\ndef accuracy(X, y):\n    predictions = LR.predict(X)\n    predsCorrect = (predictions == y).float()\n    acc = torch.mean(predsCorrect)\n    return acc\n\nWe can see that we have indeed achieved accuracy on the training data. Let’s repeat the process but now on the testing data, and look at the accuracy once more.\n\nfinalTrainAcc = accuracy(X_train, y_train)\nprint(\"Final Training Accuracy:\", finalTrainAcc.item())\n\n\n#then the testing set\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nfinalTestAcc = accuracy(X_test, y_test)\nprint(\"Final Testing Accuracy:\", finalTestAcc.item())\n\nFinal Training Accuracy: 1.0\nFinal Testing Accuracy: 0.9599999785423279\n\n\nThe results show that the model has been overfit to the training data. Despite having 100% accuracy on the training data, the model cannot reach the same level of accuracy on the testing data, only reaching about 96% as opposed to 100%. This overfitted model would not perform as well as unseen data, due to its overfitting.\n\n\nConclusion\nOverall in this blog post, through the implementation and experimentation of a fundamental algorithm of machine learning, the logistic regression model. Through three experiements: Vanilla Gradient Descent, Benefits of Momentum, and the Dangers of Overfitting, I was able to gain insights and a deeper understanding of the behavior and performance of logistic regression models as they pertain to gradient descent, observing the effects of various changes and conditions. Visualizations served as a helpful tool in order to understand and validate the models behavior. With Vanilla Gradient Descent where beta =0 we can see that convergence does indeed occur after a certain amount of iterations. Experimenting with momentum showed the benefits of higher beta values (such as beta = 0.9), in order to increase momentum, allowing for faster convergence, outperforming the first experiment of vanilla gradient descent. Fitting the model too closely to the training data showed lower accuracy on the test data, warning against overfitting. This blog post provided me a comprehensive overview and understanding of gradient descent algorithms for logistic regression, through implementation, experimentation, and visualization."
  },
  {
    "objectID": "posts/MedicalBiasJLS/ModelDoc.html",
    "href": "posts/MedicalBiasJLS/ModelDoc.html",
    "title": "Final Project Results, Model Creation Document",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport seaborn as sns\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n\nReading in our Data\n\nconditions_diabetes = pd.read_csv('conditions_diabetes.csv')\nconditions_pregnancy = pd.read_csv('conditions_pregnancy.csv')\nconditions_cancer = pd.read_csv('conditions_cancer.csv')\nconditions_heart = pd.read_csv('conditions_heart.csv')\nconditions_lungs = pd.read_csv('conditions_lungs.csv')\n\nobservations = pd.read_csv('observations_pivot.csv')\npatients = pd.read_csv('patient_clean.csv')\n\nNote: All of our datasets are grouped by related diseases (for example diabetes and comorbitidies such as diabetic retinopathy), for the rest of the post, when we say “diabetes” or “pregnancy complications,” we are talking about diabetes and all present comorbidites, or a grouping of pregnancy complications such as pre/ante eclampsia and misscarriage.\n\n\nDiabetes Modeling & Analysis\n\n1. Prepping Data\nIn order to prep our data for modelling we label encoded each of the qualitative variables (keeping track so we could decode them again later). We created a function in order to do this easily multiple times.\n\nle = LabelEncoder()\n\n# our data-prepping function for modeling\ndef prep_data(df):\n    \n    # label encode all quantitative vars\n    df[\"race\"] = le.fit_transform(df[\"race\"]) \n    race_code = {code: race for code, race in enumerate(le.classes_)}\n\n    df[\"ethnicity\"] = le.fit_transform(df[\"ethnicity\"])\n    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n\n    df[\"gender\"] = le.fit_transform(df[\"gender\"])\n    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n\n    df[\"birthplace\"] = le.fit_transform(df[\"birthplace\"])\n    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n\n    df[\"curr_town\"] = le.fit_transform(df[\"curr_town\"]) \n    curr_code = {code: bp for code, bp in enumerate(le.classes_)}\n    \n    # split data into test and train\n    train, test = train_test_split(df, test_size=0.2, random_state=42)\n    \n    X_train = train.drop(columns=['y'])\n    y_train = train['y']\n    \n    X_test = test.drop(columns=['y'])\n    y_test = test['y']\n    \n    # return split x, y, and all of the code tracking dicts\n    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code\n\n\nnp.random.seed(300)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_diabetes)\n\n\n\n2. Finding optimal model\nNext, we created a function we could reuse that identifies the best performing model on our data from the options random forest, SVC, logistic regression, and decision trees. The best model is what we use to predict the probability that each person has a certain disease (for our purposes, their risk score).\n\n# our model-finding function\ndef train_model(X_train, y_train):\n    \n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n    LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n\n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n\n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n    DTCScore  = grid_search.best_score_\n    bestDTCDepth = grid_search.best_params_\n\n\n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n\n    RFCScore  = grid_search.best_score_\n    bestRFCDepth = grid_search.best_params_\n\n    #SVC\n    SVM = SVC()\n\n    # use grid search to find best gamma for SVM\n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train, y_train)\n\n    SVMScore  = grid_search.best_score_   \n\n\n    print(\"best LR :\", LRScore)\n    print(\"best DTC:\", DTCScore)\n    print(\"best max depth: \", bestDTCDepth)\n    print(\"best RFC: \", RFCScore)\n    print(\"best max depth: \", bestRFCDepth)\n    print(\"best SVM: \", SVMScore)\n\n    # store the scores of each model\n    max_score = 0\n    max_model = \"\"\n    if LRScore &gt; max_score:\n        max_score = LRScore\n        max_model = \"LR\"\n    if DTCScore &gt; max_score:\n        max_score = DTCScore\n        max_model = \"DTC\"\n    if RFCScore &gt; max_score:\n        max_score = RFCScore\n        max_model = \"RFC\"\n    if SVMScore &gt; max_score:\n        max_score = SVMScore\n        max_model = \"SVM\"\n\n    print(\"best score overall is: \", max_score, \" with model: \", max_model)\n\n\n    \n# run model finding function on our diabetes data\nnp.random.seed(500)\ntrain_model(X_train, y_train)\n\nbest LR : 0.9050401672719269\nbest DTC: 0.9178790213124979\nbest max depth:  {'max_depth': 3}\nbest RFC:  0.9153112505043837\nbest max depth:  {'max_depth': 5}\nbest SVM:  0.9016066908770772\nbest score overall is:  0.9178790213124979  with model:  DTC\n\n\nThe results of our function should that the decision tree classifier is the best model possible, with an accuracy of 91.78%. Our accuracies tend generally lower considering the limited information we allowed the model to have, as we really wanted to see what the model would do when it predicted on identity factors such as race, ethnicity, and birthplace, and not how it would predict given information on the specific procedures and allergies a patient had.\n\n\n3. Create Risk Scores\nPredict probabilities for all our entries using the best model we found.\n\ndtc = DecisionTreeClassifier(max_depth=3)\ndtc.fit(X_train, y_train)\npred_prob = dtc.predict_proba(X_test)\n\nFor ease we created a risk finding function that can be used across factors and disease probabilities.\n\ndef find_risk(code, col, probs):\n    # finds the corresponding subset of our probability data\n    indices = (X_test[col] == code)\n    prob_subset = probs[indices]\n    # finds the average of this subset\n    av_prob = np.mean(prob_subset[:, 1]) \n    return av_prob   \n\n\n\n4. Compare Across Race, Gender, Ethnicity\nNext, we find the average risk score for different demographic characteristics: Race, Gender, and Ethnicity.\n\nRace\n\ndiabetesRaceRisk = []\n\n# find risk for each race (after finding on their code from the label encoder)\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    diabetesRaceRisk.append(newRow)\n\n# print summary table\ndiabetesRaceRisk = pd.DataFrame(diabetesRaceRisk)\ndiabetesRaceRisk = diabetesRaceRisk.sort_values(by='risk', ascending=False)\ndiabetesRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.479592\n\n\n2\nhispanic\n0.340659\n\n\n3\nwhite\n0.312536\n\n\n1\nblack\n0.256158\n\n\n\n\n\n\n\nOur model tells us that the most susceptible group to diabetes is Asian, then Hispanic and White, with Black being the least susceptible. These results were interesting in that they do indeed indicate that there may be a difference according to race, and made us think of how we could explore demographic information about Massachussetts (where our data is “from”), to understand whether these trends are reflective of larger trends.\n\n\nGender\n\ndiabetesGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    diabetesGenderRisk.append(newRow)\n\ndiabetesGenderRisk = pd.DataFrame(diabetesGenderRisk)\ndiabetesGenderRisk = diabetesGenderRisk.sort_values(by='risk', ascending=False)\ndiabetesGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.375356\n\n\n1\nM\n0.263908\n\n\n\n\n\n\n\nOur model tells us that women are slightly more likely to experience diabetes (or comorbidities) than men, which is in line with medical research we’ve seen.\n\n\nEthnicity\n\nav_risk_eth = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    av_risk_eth.append(new_row)\n\nav_risk_eth_df = pd.DataFrame(av_risk_eth)\nav_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\nav_risk_eth_df\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n2\nasian_indian\n0.714286\n\n\n13\npolish\n0.558405\n\n\n9\ngerman\n0.492674\n\n\n12\nmexican\n0.428571\n\n\n1\namerican\n0.428571\n\n\n14\nportuguese\n0.397959\n\n\n6\nenglish\n0.369491\n\n\n17\nscottish\n0.333333\n\n\n15\npuerto_rican\n0.329670\n\n\n5\ndominican\n0.328571\n\n\n0\nafrican\n0.318681\n\n\n11\nitalian\n0.314127\n\n\n3\ncentral_american\n0.306122\n\n\n7\nfrench\n0.285714\n\n\n8\nfrench_canadian\n0.261905\n\n\n4\nchinese\n0.244898\n\n\n18\nswedish\n0.205128\n\n\n16\nrussian\n0.200000\n\n\n10\nirish\n0.182902\n\n\n19\nwest_indian\n0.000000\n\n\n\n\n\n\n\nThis table gives us lots of information about risk by ethnicity, most interestingly perhaps, it agrees with our race finding that Asian people are more likely to experience diabetes, in that our most at risk ethnicity was Asian Indian. However, Chinese and West Indian, the two other Asian ethnicities in the datasest are at the bottom of the risk hierarchy, which made us consider that the risk of Asian Indian people specifically, and alone, was what was driving our race findings.\n\n\n\n5. Compare Across Wealthier & Poorer Towns of Residence/Birthplace\nIn order to compare outcomes across towns of varying socioeconomic status, we compiled a list of the richest and poorest towns present in our dataset (using Census data).\n\n# richest towns in Mass\nrichTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\", \"Chelmsford\", \"Dedham\",\n                  \"Walpole\", \"Mansfield\", \"Shrewsbury\", \"Norwood\", \"Hanover\", \"Stow\", \"Newburyport\", \"Chatham\", \"Orleans\", \"Harwich\",\n                  \"Swampscott\",\"Fairhaven\", \"Salem\"]\n\n# poorest towns in Mass\npoorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]\n\nCreate a df with all the information for the rich and poor towns\n\ndef find_town_info_row(town, bp_code_swapped, townCounts_df, code_name):\n    code = bp_code_swapped[town]\n    \n    if not townCounts_df[townCounts_df[code_name] == code].empty:\n        count = townCounts_df[townCounts_df[code_name] == code]['count'].values[0]\n    else:\n        count = 0\n    \n    new_row = {code_name: town, 'code': code, 'count': count}\n    \n    new_row_df = pd.DataFrame([new_row])\n    \n    return new_row_df\n\n\ndef find_town_info_all(counts, code_name):\n    \n    townCounts_df = pd.merge(X_test, counts, on=code_name)\n    town_info_rich = pd.DataFrame(columns=[code_name, 'code', 'count'])\n    town_info_poor = pd.DataFrame(columns=[code_name, 'code', 'count'])\n\n    bp_code_swapped = {value: key for key, value in bp_code.items()}\n\n    for town in richTowns:\n        \n        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n        town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n\n    for town in poorTowns:\n        \n        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n        town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n        \n    return town_info_rich, town_info_poor\n\nbirthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\n\ntown_info_rich, town_info_poor = find_town_info_all(birthplace_counts, 'birthplace')\n\nWe proceed with the following code to get the list of towns that sum up to 65 people from the richest towns, and 65 people from the poorest towns.\n\ndef get_towns_by_sum_pop(town_info, code_name):\n    \n    townsUsed = set()\n    peopleCount = 0\n\n    for index, row in town_info.iterrows():\n        \n        if peopleCount &gt; 65:\n            break\n        \n        name = row[code_name]\n        count = row['count']\n        townsUsed.add(name)\n        peopleCount += count\n    \n    return townsUsed, peopleCount\n\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')\n\n\n\nBirthplace\n\ndef get_av_prob_bp(townsUsed, code_name, bp_code):\n    \n    town_codes = []\n    bp_code_swapped = {value: key for key, value in bp_code.items()}\n\n\n    for town_full in townsUsed:\n        town_codes.append(bp_code_swapped[town_full])\n        \n    indices = X_test[code_name].isin(town_codes)\n    prob_subset = pred_prob[indices]\n    av_prob = np.mean(prob_subset[:, 1]) \n\n    return av_prob\n\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.33305156382079454 av_poor_prob:  0.31947027331642713\n\n\nWe find that there is not much difference in the average risk of diabetes when comparing poor and rich birthplace towns.\n\n\nCurrent Town of Residence\nCreate a dataframe with the information for rich and poor towns. Then get the list of towns that sum up to 65 people from the richest towns, and 65 people from the poorest towns.\n\ncurr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(curr_counts, 'curr_town')\n\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'curr_town')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'curr_town')\n\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n#HERE\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.25274725274725274 av_poor_prob:  0.2827087442472057\n\n\nIn this comparison, we find that people currently residing in rich towns have slightly lower rates of diabetes than those residing in poorer towns.\n\n\n\nPregnancy Analysis\nWe repeated the same exact process as above for each of our condition subsets.\nFinding the best model:\n\nnp.random.seed(567)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_pregnancy)\n\n\nnp.random.seed(567)\ntrain_model(X_train, y_train) \n\nbest LR : 0.9538094714060378\nbest DTC: 0.9632185172957705\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.9632185172957705\nbest max depth:  {'max_depth': 1}\nbest SVM:  0.9632185172957705\nbest score overall is:  0.9632185172957705  with model:  DTC\n\n\n\nCompute Average Risk scores\nPredict probabilities for all our entries using the best model we found\n\nDTC = DecisionTreeClassifier(max_depth=1)\nDTC.fit(X_train, y_train)\npred_prob = DTC.predict_proba(X_test)\n\n\n\nRace\n\npregRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    pregRaceRisk.append(newRow)\n\npregRaceRisk = pd.DataFrame(pregRaceRisk)\npregRaceRisk = pregRaceRisk.sort_values(by='risk', ascending=False)\npregRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n1\nblack\n0.051395\n\n\n2\nhispanic\n0.038217\n\n\n0\nasian\n0.037262\n\n\n3\nwhite\n0.034260\n\n\n\n\n\n\n\nHere we can see that being black gives a patient a little less than double the risk of pregnancy issues than being white. Hispanics have the second highest rate of pregnancy complications.\n\n\nGender\n\npregGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    pregGenderRisk.append(newRow)\n\npregGenderRisk = pd.DataFrame(pregGenderRisk)\npregGenderRisk = pregGenderRisk.sort_values(by='risk', ascending=False)\npregGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.074523\n\n\n1\nM\n0.000000\n\n\n\n\n\n\n\nThis result may seem a bit redundant or silly, it makes sense as generally people identified as male do not get pregnant.\n\n\nEthnicity\n\nav_risk_eth = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    av_risk_eth.append(new_row)\n\nav_risk_eth_df = pd.DataFrame(av_risk_eth)\nav_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n\n\nav_risk_eth_df\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n5\ndominican\n0.074523\n\n\n17\nscottish\n0.074523\n\n\n1\namerican\n0.054199\n\n\n3\ncentral_american\n0.053231\n\n\n19\nwest_indian\n0.049682\n\n\n8\nfrench_canadian\n0.049682\n\n\n12\nmexican\n0.049682\n\n\n14\nportuguese\n0.047908\n\n\n4\nchinese\n0.042585\n\n\n7\nfrench\n0.039746\n\n\n11\nitalian\n0.038269\n\n\n6\nenglish\n0.036060\n\n\n0\nafrican\n0.034395\n\n\n2\nasian_indian\n0.031939\n\n\n15\npuerto_rican\n0.031529\n\n\n18\nswedish\n0.029809\n\n\n10\nirish\n0.025262\n\n\n13\npolish\n0.024841\n\n\n9\ngerman\n0.023289\n\n\n16\nrussian\n0.014905\n\n\n\n\n\n\n\nHere we see that our finding that Black patients are more likely to experience pregnancy-related complications is driven largely by Dominican patients.\n\n\nBirthplace\n\nbirthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(birthplace_counts, 'birthplace')\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')\n\n\nnp.random.seed(234)\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.038981469137448335 av_poor_prob:  0.03668844154112784\n\n\n\n\nCurrent Town of Residence\n\ncurr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(curr_counts, 'curr_town')\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'curr_town')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'curr_town')\n\n\nnp.random.seed(234)\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.04586055192640981 av_poor_prob:  0.03630627027507444\n\n\nThis finding was somewhat surprising to us, in that wealthier towns were found to have higher risks of pregnancy complications. We discuss the potential implications of this result in our results section.\n\n\n\nCancer Analysis\n\nnp.random.seed(2)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_cancer)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\nnp.random.seed(500)\ntrain_model(X_train, y_train)\n\nbest LR : 0.9486775980338212\nbest DTC: 0.9546641722607386\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.9546641722607386\nbest max depth:  {'max_depth': 1}\nbest SVM:  0.9546641722607386\nbest score overall is:  0.9546641722607386  with model:  DTC\n\n\nOnce again we find that the model with the best score is DTC, The Decision Tree Classifier, with about 98% accuracy.\n\nDTC = DecisionTreeClassifier(max_depth=1)\nDTC.fit(X_train, y_train)\npred_prob = DTC.predict_proba(X_test)\n\n\nRace\n\ncancerRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    cancerRaceRisk.append(newRow)\n\ncancerRaceRisk = pd.DataFrame(cancerRaceRisk)\ncancerRaceRisk = cancerRaceRisk.sort_values(by='risk', ascending=False)\ncancerRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n3\nwhite\n0.051942\n\n\n2\nhispanic\n0.051650\n\n\n1\nblack\n0.046859\n\n\n0\nasian\n0.034009\n\n\n\n\n\n\n\nWe find across the board cancer rates are somewhat even, but that at the extremes white patients have almost a 52% risk of being classified with cancer,and Asian patients have around a 34% risk.\n\n\nGender\n\ncancerGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    cancerGenderRisk.append(newRow)\n\ncancerGenderRisk = pd.DataFrame(cancerGenderRisk)\ncancerGenderRisk = cancerGenderRisk.sort_values(by='risk', ascending=False)\ncancerGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n1\nM\n0.053825\n\n\n0\nF\n0.047147\n\n\n\n\n\n\n\nWomen are slightly less likely to have cancer.\n\n\nEthnicity\n\ncancerEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    cancerEthRisk.append(new_row)\n\ncancerEthRisk = pd.DataFrame(cancerEthRisk)\ncancerEthRisk = cancerEthRisk.sort_values(by='risk', ascending=False)\n\ncancerEthRisk\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n9\ngerman\n0.105675\n\n\n15\npuerto_rican\n0.067085\n\n\n0\nafrican\n0.067085\n\n\n4\nchinese\n0.062675\n\n\n11\nitalian\n0.059576\n\n\n6\nenglish\n0.057127\n\n\n13\npolish\n0.049934\n\n\n10\nirish\n0.049557\n\n\n14\nportuguese\n0.048342\n\n\n18\nswedish\n0.045475\n\n\n5\ndominican\n0.045475\n\n\n1\namerican\n0.041827\n\n\n3\ncentral_american\n0.034009\n\n\n7\nfrench\n0.032097\n\n\n2\nasian_indian\n0.005342\n\n\n8\nfrench_canadian\n0.005342\n\n\n12\nmexican\n0.005342\n\n\n19\nwest_indian\n0.005342\n\n\n16\nrussian\n0.005342\n\n\n17\nscottish\n0.005342\n\n\n\n\n\n\n\nOur results for ethnicity largely match the results we found distinguishing by race.\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.06399830132085002 av_poor_prob:  0.04238804096017698\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.03621368085712754 av_poor_prob:  0.05164958111475114\n\n\nWe note that for birthplace, people in rich towns are more likely to get diagnosed with cancer as opposed to people from poorer towns. For current town of residence, the opposite is true.\n\n\n\nHeart Analysis\n\nnp.random.seed(210)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_heart)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\nnp.random.seed(20)\ntrain_model(X_train, y_train)\n\n/Users/sophie/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/sophie/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nbest LR : 0.87766773045743\nbest DTC: 0.8973478595796193\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.8999156303877335\nbest max depth:  {'max_depth': None}\nbest SVM:  0.8973478595796193\nbest score overall is:  0.8999156303877335  with model:  RFC\n\n\n\nCompute Average Risk scores\nWe found that the best model to predict probabilities for all our entries in this case would be RFC.\n\nRFC = RandomForestClassifier(random_state=0, max_depth=1)\nRFC.fit(X_train, y_train)\n\npred_prob = RFC.predict_proba(X_test)\n\n\n\n4. Compare Across Race, Gender, Ethnicity\n\n\nRace\n\nheartRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    heartRaceRisk.append(newRow)\n\nheartRaceRisk = pd.DataFrame(heartRaceRisk)\nheartRaceRisk = heartRaceRisk.sort_values(by='risk', ascending=False)\nheartRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.510746\n\n\n3\nwhite\n0.502101\n\n\n1\nblack\n0.491353\n\n\n2\nhispanic\n0.491280\n\n\n\n\n\n\n\nWe find that the demographic with the highest likelihood of having heart problems is Asian, but overall the results are fairly even.\n\n\nGender\n\nheartGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    heartGenderRisk.append(newRow)\n\nheartGenderRisk = pd.DataFrame(heartGenderRisk)\nheartGenderRisk = heartGenderRisk.sort_values(by='risk', ascending=False)\nheartGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.508013\n\n\n1\nM\n0.492275\n\n\n\n\n\n\n\nAccording to our results, women and men are equally likely to have heart conditions, which disagrees with real medical trends that show men are much more likely to have these conditions.\n\n\nEthnicity\n\nheartEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    heartEthRisk.append(new_row)\n\nheartEthRisk = pd.DataFrame(heartEthRisk)\nheartEthRisk = heartEthRisk.sort_values(by='risk', ascending=False)\n\nheartEthRisk\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n2\nasian_indian\n0.556735\n\n\n17\nscottish\n0.556144\n\n\n13\npolish\n0.553649\n\n\n12\nmexican\n0.534791\n\n\n14\nportuguese\n0.519123\n\n\n1\namerican\n0.516831\n\n\n9\ngerman\n0.516270\n\n\n16\nrussian\n0.511412\n\n\n18\nswedish\n0.508385\n\n\n6\nenglish\n0.500485\n\n\n0\nafrican\n0.500460\n\n\n7\nfrench\n0.498784\n\n\n8\nfrench_canadian\n0.494611\n\n\n11\nitalian\n0.490876\n\n\n19\nwest_indian\n0.489579\n\n\n10\nirish\n0.489033\n\n\n3\ncentral_american\n0.487097\n\n\n15\npuerto_rican\n0.482365\n\n\n5\ndominican\n0.480579\n\n\n4\nchinese\n0.464757\n\n\n\n\n\n\n\nAgain, here we see very little variation.\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.10892307692307693 av_poor_prob:  0.1103076923076923\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.08661538461538462 av_poor_prob:  0.09256410256410255\n\n\nIt seems as if there are not significant differences between the risk of heart diseases between wealthier and less-wealthy birthplace towns or current towns of residence.\n\n\n\nLungs Analysis\n\nnp.random.seed(400)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_lungs)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\ntrain_model(X_train, y_train)\n\nbest LR : 0.5705770147830234\nbest DTC: 0.6107626279300099\nbest max depth:  {'max_depth': 5}\nbest RFC:  0.6210850665786289\nbest max depth:  {'max_depth': 4}\nbest SVM:  0.5971387696709585\nbest score overall is:  0.6210850665786289  with model:  RFC\n\n\n\nCompute Average Risk scores\nWe found that the best model to predict probabilities for all our entries iin this case would be RFC.\n\nRFC = RandomForestClassifier(random_state=0, max_depth=4)\nRFC.fit(X_train, y_train)\n\npred_prob = RFC.predict_proba(X_test)\n\n\n\n4. Compare Across Race, Gender, Ethnicity\n\n\nRace\n\nlungsRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    lungsRaceRisk.append(newRow)\n\nlungsRaceRisk = pd.DataFrame(lungsRaceRisk)\nlungsRaceRisk = lungsRaceRisk.sort_values(by='risk', ascending=False)\nlungsRaceRisk\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.514338\n\n\n1\nblack\n0.509092\n\n\n3\nwhite\n0.507817\n\n\n2\nhispanic\n0.492106\n\n\n\n\n\n\n\nWe find very little variation for risk rates for lung issues for race.\n\n\nGender\n\nlungsGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    lungsGenderRisk.append(newRow)\n\nlungsGenderRisk = pd.DataFrame(lungsGenderRisk)\nlungsGenderRisk = lungsGenderRisk.sort_values(by='risk', ascending=False)\nlungsGenderRisk\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.519215\n\n\n1\nM\n0.493550\n\n\n\n\n\n\n\nAgain, there are pretty even rates for gender and risk of having lung complications.\n\n\nEthnicity\n\nlungsEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    lungsEthRisk.append(new_row)\n\nlungsEthRisk = pd.DataFrame(lungsEthRisk)\nlungsEthRisk = lungsEthRisk.sort_values(by='risk', ascending=False)\n\nlungsEthRisk\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n13\npolish\n0.583031\n\n\n12\nmexican\n0.580761\n\n\n17\nscottish\n0.577455\n\n\n2\nasian_indian\n0.577379\n\n\n18\nswedish\n0.560368\n\n\n0\nafrican\n0.559980\n\n\n16\nrussian\n0.549236\n\n\n1\namerican\n0.548145\n\n\n9\ngerman\n0.521056\n\n\n14\nportuguese\n0.516368\n\n\n6\nenglish\n0.514735\n\n\n11\nitalian\n0.498196\n\n\n8\nfrench_canadian\n0.487928\n\n\n7\nfrench\n0.485426\n\n\n10\nirish\n0.481814\n\n\n15\npuerto_rican\n0.479351\n\n\n5\ndominican\n0.473627\n\n\n3\ncentral_american\n0.463493\n\n\n19\nwest_indian\n0.457941\n\n\n4\nchinese\n0.451298\n\n\n\n\n\n\n\nThis analysis follows the trend of generally even probabilities throughout, in the most extreme cases with polish people having 58% risk and chinese people having 45%.\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.4872429058942045 av_poor_prob:  0.5189382015534418\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.5039833131472166 av_poor_prob:  0.5124758651408807\n\n\nThe results for the risk score for patients of different race, gender, ethnicity, birthplace town, and current town of residence are curious as every risk score hovers around a 0.5 and generally even throughout the different demographics. This points to the conclusion that we are maybe investigating too large amount of conditions under lung ailments."
  },
  {
    "objectID": "posts/MedicalBiasJLS/AnalysisDoc.html",
    "href": "posts/MedicalBiasJLS/AnalysisDoc.html",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport seaborn as sns\nallergies = pd.read_csv('allergies.csv')\ncancer = pd.read_csv('cancer.csv')\ncareplans = pd.read_csv('careplans.csv')\nclaims = pd.read_csv('claims.csv')\nconditions = pd.read_csv('conditions.csv')\ndiabetes = pd.read_csv('diabetes.csv')\nencounters = pd.read_csv('encounters.csv') \netc = pd.read_csv('etc.csv')\nheart = pd.read_csv('heart.csv')\nimmunizations = pd.read_csv('immunizations.csv')\nlungs = pd.read_csv('lungs.csv')\nmedications = pd.read_csv('medications.csv')\nobservations = pd.read_csv('observations.csv')\npatients = pd.read_csv('patients.csv')\npregnancy = pd.read_csv('pregnancy.csv')\nprocedures = pd.read_csv('procedures.csv')"
  },
  {
    "objectID": "posts/MedicalBiasJLS/AnalysisDoc.html#exploring-general-trends-of-the-dataset",
    "href": "posts/MedicalBiasJLS/AnalysisDoc.html#exploring-general-trends-of-the-dataset",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Exploring general trends of the dataset",
    "text": "Exploring general trends of the dataset\n\nCareplans visualization\n\n# careplans viz \ncareplanCounts = careplans.groupby('DESCRIPTION').size().reset_index(name='count')\ncareplanCounts = careplanCounts.sort_values(by='count', ascending=False)\ntop10Careplans = careplanCounts.head(10)\nprint(top10Careplans)\n\nplt.figure(figsize=(10, 6))\nplt.bar(top10Careplans['DESCRIPTION'], top10Careplans['count'], color='skyblue')\nplt.xlabel('Careplan')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Careplans')\nplt.xticks(rotation=45, ha='right')\nplt.grid()\nplt.show()\n\n                              DESCRIPTION  count\n67                      Stress management    830\n34                       Exercise therapy    719\n22  Deep breathing and coughing exercises    641\n57       Recommendation to avoid exercise    641\n60                    Respiratory therapy    641\n6                     Antenatal education    609\n62                 Routine antenatal care    609\n53               Pregnancy diet education    609\n27                          Diabetic diet    498\n26          Diabetes self management plan    498\n\n\n\n\n\n\n\n\n\n\n\nVisualizing conditions dataset\n\nconditionsCounts = conditions.groupby('DESCRIPTION').size().reset_index(name='count')\nconditionsCounts = conditionsCounts.sort_values(by='count', ascending=False)\ntop10Conditions = conditionsCounts.head(10)\nprint(top10Conditions)\n\nplt.figure(figsize=(10, 6))\nplt.grid()\nplt.bar(top10Conditions['DESCRIPTION'], top10Conditions['count'], color='skyblue')\nplt.xlabel('Condition')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Conditions')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n#note that there is some matchup between top 10 careplans and top 10 conditions. this is good bc is expected behavior \n\n                              DESCRIPTION  count\n116            Viral sinusitis (disorder)   1125\n3      Acute viral pharyngitis (disorder)    602\n2             Acute bronchitis (disorder)    508\n86                            Prediabetes    458\n52                           Hypertension    373\n75                       Normal pregnancy    339\n20           Chronic sinusitis (disorder)    329\n79                           Otitis media    202\n110  Streptococcal sore throat (disorder)    146\n85                         Polyp of colon    108\n\n\n\n\n\n\n\n\n\nSinusitis seems like the most common condition in our dataset, which makes sense given its link to common colds.\n\n\nVisualizing encounters\n\n#encounters viz \nencountersCounts = encounters.groupby('DESCRIPTION').size().reset_index(name='count')\nencountersCounts = encountersCounts.sort_values(by='count', ascending=False)\ntop10Encounters = encountersCounts.head(10)\nprint(top10Encounters)\n\nplt.figure(figsize=(10, 6))\nplt.grid()\nplt.bar(top10Encounters['DESCRIPTION'], top10Encounters['count'], color='skyblue')\nplt.xlabel('Encounter Type')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Encounters')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n                    DESCRIPTION  count\n22         Outpatient Encounter   8629\n17        Encounter for symptom   2852\n24  Patient encounter procedure   1524\n27               Prenatal visit   1383\n23         Outpatient procedure   1281\n5    Consultation for treatment    899\n15     Encounter for 'check-up'    751\n13     Emergency room admission    694\n16        Encounter for problem    628\n6           Death Certification    461\n\n\n\n\n\n\n\n\n\nThe most common encounter was outpatient encounter, which makes sense as it includes general doctor visits like annuals and check-ups.\n\n\nExamining the distribution of patient identities\n\n#what the 'average' patient looks like in terms of race and gender\n\n#find most frequent race to represent average\npatientRaceCount = patients.groupby('race').size().reset_index(name='raceCount')\npatientRaceCount = patientRaceCount.sort_values(by='raceCount', ascending=False)\ntop5FreqRace = patientRaceCount.head(5)\nprint(top5FreqRace)\n\n#repeat for gender\npatientGenderCount = patients.groupby('gender').size().reset_index(name='genderCount')\npatientGenderCount = patientGenderCount.sort_values(by='genderCount', ascending=False)\ntop5FreqGender = patientGenderCount.head(5)\nprint(top5FreqGender)\n\n       race  raceCount\n3     white       1085\n2  hispanic        155\n1     black        129\n0     asian         93\n  gender  genderCount\n1      M          741\n0      F          721\n\n\nWhite patients are the most common racial group in our dataset, which may make sense for Massachussetts although this could be overrepresentation in comparison to actual demographic proportions. Male and female patients are somewhat evenly split.\n\n#visualization for what the 'average' patient looks like in terms of body stat characteristics\nobservationsRadar = observations.drop(columns = ['DATE', 'PATIENT', 'ENCOUNTER', 'CODE', 'UNITS'])\nobservationsRadar = observationsRadar.dropna()\nobservationsRadar['VALUE'] = pd.to_numeric(observationsRadar['VALUE'], errors='coerce')\naverageVals = observationsRadar.groupby('DESCRIPTION')['VALUE'].mean().reset_index()\naverageVals = averageVals[averageVals['DESCRIPTION'].isin(['Body Height', 'Body Weight', 'Body Mass Index', 'Systolic Blood Pressure', 'Diastolic Blood Pressure', 'Quality adjusted life years'])]\naverageVals\n\n\n\n\n\n\n\n\nDESCRIPTION\nVALUE\n\n\n\n\n3\nBody Height\n145.246010\n\n\n4\nBody Mass Index\n27.837863\n\n\n5\nBody Weight\n65.032186\n\n\n16\nDiastolic Blood Pressure\n84.485896\n\n\n40\nQuality adjusted life years\n41.901321\n\n\n45\nSystolic Blood Pressure\n129.587277\n\n\n\n\n\n\n\n\n#CREATION OF RADAR CHART OBTAINED FROM ONLINE CODE \ndescriptions = averageVals['DESCRIPTION'].tolist()\nvalues = averageVals['VALUE'].tolist()\n\n#to close the circle\nvalues.append(values[0])\n\nnum_vars = len(descriptions)\n\n#compute angle for each axis\nangles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n\n#need to \"complete the loop\" and append the start value to the end since plot is a cirlce\nvalues += values[:1]\nangles += angles[:1]\nvalues.pop()\n\n#plot\nfig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\nax.fill(angles, values, color='skyblue', alpha=0.7)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(descriptions, fontsize=12)\n\nax.set_yticklabels([])\n\n#add values to each point\nfor angle, value in zip(angles[:-1], values[:-1]):\n    ax.text(angle, value, str(round(value, 2)), ha='center', va='bottom', fontsize=10)\n\nplt.show()\n\n\n\n\n\n\n\n\nThis represents the average vital signs of our patients."
  },
  {
    "objectID": "posts/MedicalBiasJLS/AnalysisDoc.html#pulmonary-diseases-eda",
    "href": "posts/MedicalBiasJLS/AnalysisDoc.html#pulmonary-diseases-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Pulmonary Diseases EDA",
    "text": "Pulmonary Diseases EDA\n\nPULMONARY DISEASES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(lungs, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Pulmonary Conditions by Race')\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIt seems as though black patients are the most likely to get acute bronchitis while asian patients are most likely to get chronic sinusitis. This is important as analyzing these conditions together mike give us more mixed results in total.\n\n\nPULMONARY DISEASES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(lungs, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Pulmonary Conditions by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAgain, these vary by different diseases. However, we can see that Dominican and Mexican patients are most likely to be diagnosed with Acute bronchitis while Asian Indian patients are far more likely to get Chronic sinusitis.\n\n\nPULMONARY DISEASES BY BIRTHPLACE\nCreating a dataframe of populations of all the towns present in our data (all in Massachusetts), in order to calculate adjusted prevalance.\n\npopulations = pd.DataFrame()\nunique_birthplaces = lungs['birthplace'].unique()\npopulations['birthplace'] = unique_birthplaces\npopulations['pop'] = [37819, 101727, 35313, 93682, 3785, 53896, 87954,\n                       62698, 60803, 5025, 18317, 23662, 1734, 79762, 205319, 1641,\n                       25334, 650706, 6362, 31531, 59922, 6802, 58528, 87381, 19872,\n                       17612, 54980, 25121, 24747, 17489, 35022, 64065, 22992, 6196, 15946, 27395,\n                       44722, 25905, 9806, 11657, 65399, 8485, 15988, 113608, 28854, 4301, 104826, \n                       118488, 29155, 13885, 12265, 23315, 1677, 28501, 67153, 49350, 29327, 10293,\n                       29349, 8270, 154064, 19808, 43310, 29862, 22325, 16593, 6388, 6379, 17619, 14939, \n                       16720, 40971, 31747, 43646, 25050, 38637, 1802, 15702, 15101, 29195, 18448, 16732,\n                       16516, 11048, 7754, 11066, 19948, 4688, 100891, 33792, 18181, 11115, 37286, 100682,\n                       36229, 14570, 46601, 10911, 11625, 17669, 13697, 20902, 10667, 19163, 22666, 10580,\n                       32158, 11777, 1861, 2215, 6347, 64712, 23629, 34307, 752, 9547, 16705, 13866, 18510,\n                       43784, 35744, 21478, 70963, 42844, 40535, 16296, 16127, 7973, 31296, 26123, 3265, 13320,\n                       53241, 3234, 7839, 14749, 8055, 24498, 9640, 42235, 1730, 5798, 37973, 8316, 23923, 18662,\n                       10084, 6975, 15827, 49532, 11964, 136913, 6279, 7214, 17806, 41248, 6358, 10874, 19063, 6569,\n                       23184, 11753, 11386, 2985, 16450, 9182, 7764, 21374, 11802, 41502, 4111, 16053, 6183, 27003,\n                       15710, 6125, 8471, 4963, 29836, 1029, 15227, 10000, 12337, 1793, 491, 4678, 11988, 8168, 28950,\n                       1566, 12904, 57410, 6850, 13427, 11327, 6532, 917, 17456, 25209, 717, 17182, 15168, 12777, \n                       8153, 14313, 13435, 1264, 7884, 9230, 14180, 3056, 12418, 14382, 31248, 10169, 8541, 16188, \n                       36500, 31388, 17027, 16094, 13911, 28385, 9395, 11261, 1489, 12925, 27999, 5943, 16693, 5346,\n                       9811, 27400, 4871, 24296, 12133, 6346, 31635, 11688, 1245, 5284, 5966, 1458, 780, 27135, 316,\n                       5429, 17765, 12629, 3390, 6952, 7144, 2180, 7649, 4907, 5139, 4852, 5125, 5135, 2901, 5398, 4519]\n\n\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\nconditions_summed = lungs.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatalungs = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatalungs[f'prevalence_{condition_column}'] = prevdatalungs[condition_column] / prevdatalungs['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatalungs[prevdatalungs[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn many of these conditions, it’s worth nothing that one town in particular has a much higher prevelance. Unfortunately, with the vast number of towns and conditions there are so many it is difficult to accurately visualize. However, Hancock is extremely high for Sinusitus. This could be notable as Hancock ranked as the 287th richest town in Mass out of 341, so it is less wealthy compares to others in our dataset."
  },
  {
    "objectID": "posts/MedicalBiasJLS/AnalysisDoc.html#cancer-eda",
    "href": "posts/MedicalBiasJLS/AnalysisDoc.html#cancer-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Cancer EDA",
    "text": "Cancer EDA\n\nCANCER BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(cancer, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Cancer by Race')\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAsian patients seem to dominate many categories.\n\n\nCANCER BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(cancer, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cancer by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCANCER PREVALANCE BY BIRTHPLACE\n\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\nconditions_summed = cancer.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatacanc = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatacanc[f'prevalence_{condition_column}'] = prevdatacanc[condition_column] / prevdatacanc['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatacanc[prevdatacanc[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor some cancer types, very few places record incidents. It is harder to draw conclusion for different conditions with this lack of data."
  },
  {
    "objectID": "posts/MedicalBiasJLS/AnalysisDoc.html#diabetes-eda",
    "href": "posts/MedicalBiasJLS/AnalysisDoc.html#diabetes-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Diabetes EDA",
    "text": "Diabetes EDA\n\nDIABETES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(diabetes, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Diabetes by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDIABETES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(diabetes, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Diabetes by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe don’t see significant race of ethnicity disparities for the different diabetes conditions.\n\n\nDIABETES PREVALENCE BY BIRTHPLACE\n\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\nconditions_summed = diabetes.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatadiab = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatadiab[f'prevalence_{condition_column}'] = prevdatadiab[condition_column] / prevdatadiab['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatadiab[prevdatadiab[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere there is a mix of some towns dominating prevelance, and some diabetes cormorbidities being generally quite rare."
  },
  {
    "objectID": "posts/MedicalBiasJLS/AnalysisDoc.html#cardiovascular-diseases-eda",
    "href": "posts/MedicalBiasJLS/AnalysisDoc.html#cardiovascular-diseases-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Cardiovascular Diseases EDA",
    "text": "Cardiovascular Diseases EDA\n\nCARDIOVASCULAR DISEASES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(heart, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Cardiovascular Diseases by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\nHispanic patients seem slightly more common in our heart condition dataset.\n\n\nCARDIOVASCULAR DISEASES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(heart, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cardiovascular Diseases by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCARDIOVASCULAR DISEASE PREVALENCE BY BIRTHPLACE\n\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\nconditions_summed = heart.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdataheart = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdataheart[f'prevalence_{condition_column}'] = prevdataheart[condition_column] / prevdataheart['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdataheart[prevdataheart[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharlemont and Russell seem to stand out with high levels of heart defects, with a wealth rating of 306 and 280 out of 341 respectively."
  },
  {
    "objectID": "posts/MedicalBiasJLS/AnalysisDoc.html#pregnancy-complicationsn-eda",
    "href": "posts/MedicalBiasJLS/AnalysisDoc.html#pregnancy-complicationsn-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Pregnancy Complicationsn EDA",
    "text": "Pregnancy Complicationsn EDA\n\nPREGNANCY COMPLICATIONS BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(pregnancy, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Pregnancy Complications by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\nGenerally non-white patients seem to have generally higher risks here.\n\n\nPREGNANCY COMPLICATIONS BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(pregnancy, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cardiovascular Pregnancy Complications by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWest-Indian patients seem to dominate a few of these conditions.\n\n\nPREGNANCY COMPLICATIONS BY BIRTHPLACE\n\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\nconditions_summed = pregnancy.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatapreg = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatapreg[f'prevalence_{condition_column}'] = prevdatapreg[condition_column] / prevdatapreg['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatapreg[prevdatapreg[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor many here as well there are few towns reporting incidents of these conditions, which could make analysis harder in terms of generalizing trends."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451: Mid-Course Reflection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSCI 0451: Reflective Goal-Setting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCSCI 0451: End-Of-Course Reflection\n\n\n\n\n\nWe reflect on our learning, engagement, and achievement over the course of the semester. \n\n\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes\n\n\n\n\n\nFinal Project Blog Post\n\n\n\n\n\nMay 9, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\nBlog Post 5: Implementing Logistic Regression\n\n\n\n\n\nApr 12, 2024\n\n\nJulia Joy\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Decision Making\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2024\n\n\nJulia Joy\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nBlog Post 4: Implementing and testing the perceptron algorithm.\n\n\n\n\n\nMar 31, 2024\n\n\nJulia Joy\n\n\n\n\n\n\n\n\n\n\n\n\nWiDS Conference at Middlebury College\n\n\n\n\n\nBlog Post 3: Refelcting on and contextualizing the learning obtained from the Women in Data Science Conference\n\n\n\n\n\nMar 4, 2024\n\n\nJulia Joy\n\n\n\n\n\n\n\n\n\n\n\n\nDissecting Racial Bias\n\n\n\n\n\nBlog Post 2: Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nFeb 29, 2024\n\n\nJulia Joy\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\nBlog Post 1: Palmer Penguins\n\n\n\n\n\nFeb 18, 2024\n\n\nJulia Joy\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Methods, Data Cleaning Document\n\n\n\n\n\nProcess of data cleaning.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Results, Model Creation Document\n\n\n\n\n\nProcess of model creation.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Methods, Exploratory Data Analysis\n\n\n\n\n\nVisualizing the data we used.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\nNo matching items"
  }
]